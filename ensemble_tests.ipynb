{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from read_script import read_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes an array of dict_data\n",
    "# and combines embeddings by averaging with self then appending with other\n",
    "# convers to array with labels and returns\n",
    "def ensemble_append_mean_reps(dicts, labels):\n",
    "    \n",
    "    new_dict = dict()\n",
    "    keys = dicts[0].keys()\n",
    "    for key in keys:\n",
    "        seqs = []\n",
    "        for d in dicts:\n",
    "            seq = np.mean(d[key], axis=0)\n",
    "            seqs.append(seq)\n",
    "        combined_seqs = np.concatenate(seqs, axis=0)\n",
    "        new_dict[key] = combined_seqs\n",
    "        \n",
    "    emb_size = list(new_dict.values())[0].shape[0]\n",
    "    X = np.zeros((len(new_dict), emb_size))\n",
    "    y = np.zeros(len(new_dict))\n",
    "    \n",
    "    i = 0\n",
    "    for key in new_dict:\n",
    "        X[i] = np.mean(new_dict[key], axis=0)\n",
    "        y[i] = labels[key]\n",
    "        i += 1\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 15 s, total: 28.3 s\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "task = 'remote_homology'\n",
    "\n",
    "X_train_e = read_dataset('elmo', task, 'train')\n",
    "X_train_u = read_dataset('unirep', task, 'train')\n",
    "y_train_dict = read_dataset('label', task, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12305, 2924)\n",
      "CPU times: user 4.89 s, sys: 414 ms, total: 5.31 s\n",
      "Wall time: 5.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_app, y_train = ensemble_append_mean_reps([X_train_e, X_train_u], y_train_dict)\n",
    "print(X_train_app.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fl1092/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/fl1092/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train_app, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append model valid score:  0.03814713896457766\n"
     ]
    }
   ],
   "source": [
    "X_valid_e = read_dataset('elmo', task, 'valid')\n",
    "X_valid_u = read_dataset('unirep', task, 'valid')\n",
    "y_valid_dict = read_dataset('label', task, 'valid')\n",
    "X_valid_app, y_valid = ensemble_append_mean_reps([X_valid_e, X_valid_u], y_valid_dict)\n",
    "\n",
    "valid_score = clf.score(X_valid_app, y_valid)\n",
    "print(f\"append model valid score: \", valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append model test score:  0.054317548746518104\n"
     ]
    }
   ],
   "source": [
    "X_test_e = read_dataset('elmo', task, 'test')\n",
    "X_test_u = read_dataset('unirep', task, 'test')\n",
    "y_test_dict = read_dataset('label', task, 'test')\n",
    "X_test_app, y_test = ensemble_append_mean_reps([X_test_e, X_test_u], y_test_dict)\n",
    "\n",
    "test_score = clf.score(X_test_app, y_test)\n",
    "print(f\"append model test score: \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 43min 52s, sys: 1min 13s, total: 1h 45min 5s\n",
      "Wall time: 2h 53min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_app = OneVsRestClassifier(LinearSVC())\n",
    "clf_app.fit(X_train_app, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append model valid score:  0.03678474114441417\n"
     ]
    }
   ],
   "source": [
    "X_valid_e = read_dataset('elmo', task, 'valid')\n",
    "X_valid_u = read_dataset('unirep', task, 'valid')\n",
    "y_valid_dict = read_dataset('label', task, 'valid')\n",
    "X_valid_app, y_valid = ensemble_append_mean_reps([X_valid_e, X_valid_u], y_valid_dict)\n",
    "\n",
    "valid_score = clf_app.score(X_valid_app, y_valid)\n",
    "print(f\"append model valid score: \", valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append model test score:  0.057103064066852366\n"
     ]
    }
   ],
   "source": [
    "X_test_e = read_dataset('elmo', task, 'test')\n",
    "X_test_u = read_dataset('unirep', task, 'test')\n",
    "y_test_dict = read_dataset('label', task, 'test')\n",
    "X_test_app, y_test = ensemble_append_mean_reps([X_test_e, X_test_u], y_test_dict)\n",
    "\n",
    "test_score = clf_app.score(X_test_app, y_test)\n",
    "print(f\"append model test score: \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
