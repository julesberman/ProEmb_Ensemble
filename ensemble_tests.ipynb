{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run me first!\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from read_scripts import dict_2_arr \n",
    "from read_scripts import read_dataset \n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "task = \"remote_homology\"\n",
    "\n",
    "def fit_logistic(X, y):\n",
    "    start = timer()\n",
    "    clf = LogisticRegression(max_iter=5000)\n",
    "    clf.fit(X, y)\n",
    "    end = timer()\n",
    "    print(f\"fit time: \", timedelta(seconds=end-start))\n",
    "    \n",
    "    train_score = clf.score(X, y)\n",
    "    print(f\"model train score: \", train_score)\n",
    "    \n",
    "    # when run will play a ping sound!\n",
    "    os.system(\"printf '\\a'\")\n",
    "    os.system(\"printf '\\a'\")\n",
    "    os.system(\"printf '\\a'\")\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes an array of dict_data\n",
    "# and combines embeddings by averaging with self then appending with other\n",
    "# convers to array with labels and returns\n",
    "def ensemble_append_mean_reps(dicts, labels):\n",
    "    \n",
    "    new_dict = dict()\n",
    "    keys = dicts[0].keys()\n",
    "    for key in keys:\n",
    "        seqs = []\n",
    "        for d in dicts:\n",
    "            seq = np.mean(d[key], axis=0)\n",
    "            seq = preprocessing.normalize([seq], norm='l2')\n",
    "            seqs.append(seq)\n",
    "        combined_seqs = np.concatenate(seqs, axis=1)\n",
    "        new_dict[key] = combined_seqs\n",
    "\n",
    "    emb_size = list(new_dict.values())[0].shape[1]\n",
    "    X = np.zeros((len(new_dict), emb_size))\n",
    "    y = np.zeros(len(new_dict))\n",
    "    \n",
    "    i = 0\n",
    "    for key in new_dict:\n",
    "        X[i] = new_dict[key]\n",
    "        y[i] = labels[key]\n",
    "        i += 1\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = read_dataset('label', task, \"train\")\n",
    "X_train_e = read_dataset('elmo', task, \"train\")\n",
    "X_train_u = read_dataset('unirep', task, \"train\")\n",
    "X_train_t = read_dataset('transformer', task, \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12305, 3692)\n",
      "(12305,)\n"
     ]
    }
   ],
   "source": [
    "X_train_app, y_train = ensemble_append_mean_reps([X_train_e, X_train_u, X_train_t], y_train)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train_app)\n",
    "X_train_app = scaler.transform(X_train_app)\n",
    "\n",
    "print(X_train_app.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time:  0:43:56.453212\n"
     ]
    }
   ],
   "source": [
    "fit_logistic(X_train_app, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append model valid score:  0.3746594005449591\n",
      "append model test_fold_holdout score:  0.2743732590529248\n",
      "append model test_superfamily_holdout score:  0.46485623003194887\n",
      "append model test_family_holdout score:  0.9544025157232704\n"
     ]
    }
   ],
   "source": [
    "for split in ['valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']:\n",
    "    X_test_e = read_dataset('elmo', task, split)\n",
    "    X_test_u = read_dataset('unirep', task, split)\n",
    "    X_test_t = read_dataset('transformer', task, split)\n",
    "    y_test_dict = read_dataset('label', task, split)\n",
    "    X_test_app, y_test = ensemble_append_mean_reps([X_test_e, X_test_u, X_test_t], y_test_dict)\n",
    "    \n",
    "    X_test_app = scaler.transform(X_test_app)\n",
    "    test_score = clf.score(X_test_app, y_test)\n",
    "    print(f\"append model {split} score: \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVD_reduce_dim(m, dim):\n",
    "    need_full = False\n",
    "    \n",
    "    # compute svd\n",
    "    U, S, _ = np.linalg.svd(m)\n",
    "    \n",
    "    # multiply U against Sigma\n",
    "    reduced = np.matmul(U[:,:dim], np.diag(S[:dim]))\n",
    "    \n",
    "    return reduced\n",
    "\n",
    "# work in progress\n",
    "def ensemble_SVD_concat(matrices, dim):\n",
    "    \n",
    "    # concatenate samples\n",
    "    combined = np.concatenate(matrices, axis=1)\n",
    "    \n",
    "    # reduce dimension\n",
    "    reduced = SVD_reduce_dim(combined, dim)\n",
    "    \n",
    "    print(f\"reduced shape {reduced.shape}\")\n",
    "    \n",
    "    return reduced\n",
    "\n",
    "\n",
    "# # work in progress\n",
    "# def ensemble_SVD_orthogonal_procrustes_reps(m1, m2, min_dim=99999999):\n",
    "    \n",
    "#     print(f\"IN m1 shape: {m1.shape}, m2 shape: {m2.shape}\")\n",
    "    \n",
    "#     min_dim = min(min(min_dim, m1.shape[1]), m2.shape[1])\n",
    "#     print(\"reducing to dim\", min_dim)\n",
    "    \n",
    "#     if m1.shape[1] > min_dim:\n",
    "\n",
    "#         m1 = PCA(n_components=min_dim).fit(m1).transform(m1)\n",
    "#         print(m1.shape)\n",
    "        \n",
    "#     if m2.shape[1] > min_dim:\n",
    "#         m2 = PCA(n_components=min_dim).fit(m2).transform(m2)\n",
    "#         print(m2.shape)\n",
    "        \n",
    "#     print(f\"OUT m1 shape: {m1.shape}, m2 shape: {m2.shape}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced shape (16281, 650)\n",
      "(16281, 650)\n",
      "(12305, 650)\n",
      "(12305,)\n"
     ]
    }
   ],
   "source": [
    "dim = 650\n",
    "ys = []\n",
    "els = []\n",
    "trfs = []\n",
    "unis = []\n",
    "\n",
    "for split in ['train', 'valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']:\n",
    "    \n",
    "    y_dict = read_dataset('label', task, split)\n",
    "    X_e, y = dict_2_arr(read_dataset('elmo', task, split), y_dict)\n",
    "    X_t, y = dict_2_arr(read_dataset('transformer', task, split), y_dict)\n",
    "    X_u, y = dict_2_arr(read_dataset('unirep', task, split), y_dict)\n",
    "    \n",
    "    \n",
    "    ys.append(y)\n",
    "    els.append(X_e)  \n",
    "    trfs.append(X_t)\n",
    "    unis.append(X_u)\n",
    "    \n",
    "e_corpus = np.concatenate(els, axis=0)\n",
    "t_corpus = np.concatenate(trfs, axis=0)\n",
    "u_corpus = np.concatenate(unis, axis=0)\n",
    "                          \n",
    "e_corpus = preprocessing.normalize(e_corpus, norm='l2')\n",
    "t_corpus = preprocessing.normalize(t_corpus, norm='l2')\n",
    "u_corpus = preprocessing.normalize(u_corpus, norm='l2')\n",
    "                          \n",
    "combine_corpus = ensemble_SVD_concat([e_corpus, t_corpus, u_corpus], dim)\n",
    "print(combine_corpus.shape)\n",
    "\n",
    "X_train = combine_corpus[:len(ys[0])]\n",
    "y_train = ys[0]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time:  0:03:00.446279\n",
      "model train score:  1.0\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# scale\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# fit!\n",
    "clf = fit_logistic(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12305 13039\n",
      "valid\n",
      "(734, 650)\n",
      "(734,)\n",
      "conact then SVD-650 model valid score:  0.37329700272479566\n",
      "13039 13757\n",
      "test_fold_holdout\n",
      "(718, 650)\n",
      "(718,)\n",
      "conact then SVD-650 model test_fold_holdout score:  0.24512534818941503\n",
      "13757 15009\n",
      "test_superfamily_holdout\n",
      "(1252, 650)\n",
      "(1252,)\n",
      "conact then SVD-650 model test_superfamily_holdout score:  0.4472843450479233\n",
      "15009 16281\n",
      "test_family_holdout\n",
      "(1272, 650)\n",
      "(1272,)\n",
      "conact then SVD-650 model test_family_holdout score:  0.9520440251572327\n"
     ]
    }
   ],
   "source": [
    "remain_splits = ['valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']\n",
    "end = len(ys[0])\n",
    "\n",
    "for i in range(len(remain_splits)):\n",
    "    split = remain_splits[i]\n",
    "    start = end\n",
    "    end = len(ys[i+1])+start\n",
    "    print(start,end)\n",
    "    X = combine_corpus[start:end]\n",
    "    y = ys[i+1]\n",
    "    \n",
    "    print(split)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    test_score = clf.score(X, y)\n",
    "    print(f\"conact then SVD-{dim} model {split} score: \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_conact_SVD(dim):\n",
    "#     print(f\"=========================Concat-SVD {dim}=============================\\n\")\n",
    "#     ys = []\n",
    "#     els = []\n",
    "#     trfs = []\n",
    "#     unis = []\n",
    "\n",
    "#     print(\"\")\n",
    "#     for split in ['train', 'valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']:\n",
    "\n",
    "#         y_dict = read_dataset('label', task, split)\n",
    "#         X_e, y = dict_2_arr(read_dataset('elmo', task, split), y_dict)\n",
    "#         X_t, y = dict_2_arr(read_dataset('transformer', task, split), y_dict)\n",
    "#         X_u, y = dict_2_arr(read_dataset('unirep', task, split), y_dict)\n",
    "\n",
    "\n",
    "#         ys.append(y)\n",
    "#         els.append(X_e)  \n",
    "#         trfs.append(X_t)\n",
    "#         unis.append(X_u)\n",
    "\n",
    "#     e_corpus = np.concatenate(els, axis=0)\n",
    "#     t_corpus = np.concatenate(trfs, axis=0)\n",
    "#     u_corpus = np.concatenate(unis, axis=0)\n",
    "\n",
    "#     e_corpus = preprocessing.normalize(e_corpus, norm='l2')\n",
    "#     t_corpus = preprocessing.normalize(t_corpus, norm='l2')\n",
    "#     u_corpus = preprocessing.normalize(u_corpus, norm='l2')\n",
    "\n",
    "#     combine_corpus = ensemble_SVD_concat([e_corpus, t_corpus, u_corpus], dim)\n",
    "\n",
    "#     X_train = combine_corpus[:len(ys[0])]\n",
    "#     y_train = ys[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
