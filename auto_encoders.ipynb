{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from read_scripts import dict_2_arr \n",
    "from read_scripts import read_dataset \n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "task = \"remote_homology\"\n",
    "\n",
    "def fit_logistic(X, y):\n",
    "    start = timer()\n",
    "    clf = LogisticRegression(max_iter=5000)\n",
    "    clf.fit(X, y)\n",
    "    end = timer()\n",
    "    print(f\"fit time: \", timedelta(seconds=end-start))\n",
    "    \n",
    "    train_score = clf.score(X, y)\n",
    "    print(f\"model train score: \", train_score)\n",
    "    \n",
    "    # when run will play a ping sound!\n",
    "    os.system(\"printf '\\a'\")\n",
    "    os.system(\"printf '\\a'\")\n",
    "    os.system(\"printf '\\a'\")\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_model_normalized_corpus(model):\n",
    "    ys = []\n",
    "    Xs = []\n",
    "\n",
    "    # read all splits\n",
    "    for split in ['train', 'valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']:\n",
    "\n",
    "        y_dict = read_dataset('label', task, split)\n",
    "        X, y = dict_2_arr(read_dataset(model, task, split), y_dict)\n",
    "        \n",
    "        ys.append(y)\n",
    "        Xs.append(X)  \n",
    "    \n",
    "    # concat all splits\n",
    "    corpus = np.concatenate(Xs, axis=0)\n",
    "    \n",
    "    return corpus, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_normalized_corpus():\n",
    "    ys = []\n",
    "    els = []\n",
    "    trfs = []\n",
    "    unis = []\n",
    "\n",
    "    # read all splits\n",
    "    for split in ['train', 'valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']:\n",
    "\n",
    "        y_dict = read_dataset('label', task, split)\n",
    "        X_e, y = dict_2_arr(read_dataset('elmo', task, split), y_dict)\n",
    "        X_t, y = dict_2_arr(read_dataset('transformer', task, split), y_dict)\n",
    "        X_u, y = dict_2_arr(read_dataset('unirep', task, split), y_dict)\n",
    "\n",
    "        ys.append(y)\n",
    "        els.append(X_e)  \n",
    "        trfs.append(X_t)\n",
    "        unis.append(X_u)\n",
    "    \n",
    "    # concat all splits\n",
    "    e_corpus = np.concatenate(els, axis=0)\n",
    "    t_corpus = np.concatenate(trfs, axis=0)\n",
    "    u_corpus = np.concatenate(unis, axis=0)\n",
    "    \n",
    "    # normalize each indvidually \n",
    "    e_corpus = preprocessing.normalize(e_corpus, norm='l2')\n",
    "    t_corpus = preprocessing.normalize(t_corpus, norm='l2')\n",
    "    u_corpus = preprocessing.normalize(u_corpus, norm='l2')\n",
    "    \n",
    "    # concatenate all corpuses\n",
    "    combined = np.concatenate([e_corpus, t_corpus, u_corpus], axis=1)\n",
    "    \n",
    "    return combined, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_over_corpus(corpus, ys, model_name):\n",
    "\n",
    "    all_score = {}\n",
    "\n",
    "    # get out training slice\n",
    "    X_train = corpus[:len(ys[0])]\n",
    "    y_train = ys[0]\n",
    "\n",
    "    # scale\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "\n",
    "    # fit!\n",
    "    clf = fit_logistic(X_train, y_train)\n",
    "\n",
    "    # record train score\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    print(f\"{model_name} train score: \", train_score)\n",
    "\n",
    "    all_score[\"train\"]  = train_score\n",
    "\n",
    "    # get slices for remaing splits and score\n",
    "    remain_splits = ['valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']\n",
    "    end = len(ys[0])\n",
    "    for i in range(len(remain_splits)):\n",
    "        split = remain_splits[i]\n",
    "        start = end\n",
    "        end = len(ys[i+1])+start\n",
    "\n",
    "        X = corpus[start:end]\n",
    "        y = ys[i+1]\n",
    "\n",
    "        X = scaler.transform(X)\n",
    "\n",
    "        test_score = clf.score(X, y)\n",
    "\n",
    "        all_score[split]  = test_score\n",
    "\n",
    "        print(f\"{model_name} {split} score: \", test_score)\n",
    "\n",
    "    return all_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_auto_encoder(X, latent_dim, hidden_layers):\n",
    "\n",
    "    n_input = X.shape[1]\n",
    "\n",
    "    full_layers = hidden_layers + [latent_dim] + hidden_layers[::-1]\n",
    "    print(\"layers\", full_layers)\n",
    "    \n",
    "    reg = MLPRegressor(hidden_layer_sizes = full_layers, \n",
    "                       activation = 'relu', \n",
    "                       solver = 'adam', \n",
    "                       learning_rate_init = 0.0001, \n",
    "                       max_iter = 100, \n",
    "                       tol = 0.0000001, \n",
    "                       verbose = True)\n",
    "    \n",
    "    print(\"fitting auto_encoder...\")\n",
    "    reg.fit(X, X)\n",
    "    \n",
    "    auto_encoder_train_score = reg.score(X, X)\n",
    "    print(\"auto_encoder_train_score:\", auto_encoder_train_score)\n",
    "    return reg\n",
    "\n",
    "\n",
    "def encoder(X, reg):\n",
    "    print(\"encoding...\")\n",
    "    X = np.asmatrix(X)\n",
    "    \n",
    "    encoder1 = X*reg.coefs_[0] + reg.intercepts_[0]\n",
    "    encoder1 = (np.exp(encoder1) - np.exp(-encoder1))/(np.exp(encoder1) + np.exp(-encoder1))\n",
    "    \n",
    "    encoder2 = encoder1*reg.coefs_[1] + reg.intercepts_[1]\n",
    "    encoder2 = (np.exp(encoder2) - np.exp(-encoder2))/(np.exp(encoder2) + np.exp(-encoder2))\n",
    "    \n",
    "    latent = encoder2*reg.coefs_[2] + reg.intercepts_[2]\n",
    "    latent = (np.exp(latent) - np.exp(-latent))/(np.exp(latent) + np.exp(-latent))\n",
    "    \n",
    "    print(\"encoded as: \", latent.shape)\n",
    "    \n",
    "    return np.asarray(latent)\n",
    "\n",
    "def train_and_encode(X, latent_dim=200, hidden_layers=[500, 300]):\n",
    "    \n",
    "    reg = train_auto_encoder(X, latent_dim, hidden_layers)\n",
    "    X_encoded = encoder(X, reg)\n",
    "    \n",
    "    return X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_e, ys = get_single_model_normalized_corpus('elmo')\n",
    "X_t, ys = get_single_model_normalized_corpus('transformer')\n",
    "X_u, ys = get_single_model_normalized_corpus('unirep')\n",
    "\n",
    "X_t_encoded = train_and_encode(X_t)\n",
    "X_u_encoded = train_and_encode(X_u)\n",
    "X_e_encoded = train_and_encode(X_e)\n",
    "\n",
    "conact_encoded = np.concatenate([X_t_encoded, X_u_encoded, X_e_encoded], axis=1)\n",
    "\n",
    "scores = train_test_over_corpus(conact_encoded, ys, \"auto_encode_then_concat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, ys = get_combined_normalized_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers [2000, 1000, 500, 200, 500, 1000, 2000]\n",
      "fitting auto_encoder...\n",
      "Iteration 1, loss = 0.00273546\n",
      "Iteration 2, loss = 0.00258025\n",
      "Iteration 3, loss = 0.00255058\n",
      "Iteration 4, loss = 0.00253404\n",
      "Iteration 5, loss = 0.00252201\n",
      "Iteration 6, loss = 0.00251289\n",
      "Iteration 7, loss = 0.00250583\n",
      "Iteration 8, loss = 0.00250029\n",
      "Iteration 9, loss = 0.00249586\n",
      "Iteration 10, loss = 0.00249227\n",
      "Iteration 11, loss = 0.00248926\n",
      "Iteration 12, loss = 0.00248679\n",
      "Iteration 13, loss = 0.00248467\n",
      "Iteration 14, loss = 0.00248287\n",
      "Iteration 15, loss = 0.00248134\n",
      "Iteration 16, loss = 0.00248000\n",
      "Iteration 17, loss = 0.00247886\n",
      "Iteration 18, loss = 0.00247785\n",
      "Iteration 19, loss = 0.00247697\n",
      "Iteration 20, loss = 0.00247617\n",
      "Iteration 21, loss = 0.00247545\n",
      "Iteration 22, loss = 0.00247478\n",
      "Iteration 23, loss = 0.00247416\n",
      "Iteration 24, loss = 0.00247358\n",
      "Iteration 25, loss = 0.00247301\n",
      "Iteration 26, loss = 0.00247246\n",
      "Iteration 27, loss = 0.00247191\n",
      "Iteration 28, loss = 0.00247136\n",
      "Iteration 29, loss = 0.00247081\n",
      "Iteration 30, loss = 0.00247020\n",
      "Iteration 31, loss = 0.00246957\n",
      "Iteration 32, loss = 0.00246890\n",
      "Iteration 33, loss = 0.00246819\n",
      "Iteration 34, loss = 0.00246744\n",
      "Iteration 35, loss = 0.00246662\n",
      "Iteration 36, loss = 0.00246575\n",
      "Iteration 37, loss = 0.00246482\n",
      "Iteration 38, loss = 0.00246383\n",
      "Iteration 39, loss = 0.00246279\n",
      "Iteration 40, loss = 0.00246165\n",
      "Iteration 41, loss = 0.00246048\n",
      "Iteration 42, loss = 0.00245929\n",
      "Iteration 43, loss = 0.00245804\n",
      "Iteration 44, loss = 0.00245673\n",
      "Iteration 45, loss = 0.00245539\n",
      "Iteration 46, loss = 0.00245402\n",
      "Iteration 47, loss = 0.00245259\n",
      "Iteration 48, loss = 0.00245119\n",
      "Iteration 49, loss = 0.00244969\n",
      "Iteration 50, loss = 0.00244821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julesberman/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss:  0.7976091125686712\n",
      "encoding...\n",
      "encoded as:  (16281, 500)\n",
      "fit time:  0:10:03.322228\n",
      "model train score:  1.0\n",
      "Done!\n",
      "concat_then_auto_encode train score:  1.0\n",
      "concat_then_auto_encode valid score:  0.35967302452316074\n",
      "concat_then_auto_encode test_fold_holdout score:  0.25487465181058494\n",
      "concat_then_auto_encode test_superfamily_holdout score:  0.4353035143769968\n",
      "concat_then_auto_encode test_family_holdout score:  0.934748427672956\n"
     ]
    }
   ],
   "source": [
    "X = train_and_encode(X, hidden_layers=[2000, 1000, 500])\n",
    "\n",
    "scores = train_test_over_corpus(X, ys, \"concat_then_auto_encode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.layers import Lambda, Concatenate\n",
    "\n",
    "def build_autoencoder(X):\n",
    "    input_dim = X.shape[1]\n",
    "    x = keras.Input(shape=(input_dim,))\n",
    "    encoded = layers.Dense(500, activation='relu')(x)\n",
    "    encoded = layers.Dense(300, activation='relu')(encoded)\n",
    "    encoded = layers.Dense(latent_dim, activation='relu')(encoded)\n",
    "    decoded = layers.Dense(300, activation='relu')(encoded)\n",
    "    decoded = layers.Dense(500, activation='relu')(decoded)\n",
    "    decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    encoder = keras.Model(x1, encoded1)  \n",
    "    return encoder, decoded\n",
    "\n",
    "def build_DAEME(X1, X2, X3, latent_dim):\n",
    "    \n",
    "\n",
    "    encoder1, decoded1 = build_autoencoder(X1)\n",
    "    \n",
    "\n",
    "    encoder1 = keras.Model(x1, encoded1)\n",
    "    encoder2 = keras.Model(x2, encoded2)\n",
    "    \n",
    "    encoder3 = keras.x3(x_full, encoded3)\n",
    "    \n",
    "    encoder_full = keras.Model(x_full, encoded_full)\n",
    "    auto_encoder = keras.Model(x_full, decoded_full)    \n",
    "    \n",
    "    def DAEME_loss(e1, e2, e3, x1, x2, x3):\n",
    "\n",
    "        def loss(y_true, y_pred):\n",
    "            \n",
    "            x1_e = e1.predict(x1)\n",
    "            x2_e = e2.predict(x2)\n",
    "            x3_e = e3.predict(x3)\n",
    "            \n",
    "            return K.sum(K.square(x1_e - x2_e), axis=-1)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    auto_encoder.compile(optimizer='adam',\n",
    "                  loss=DAEME_loss(encoder1, encoder2, encoder3, x1, x2, x3),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return auto_encoder, encoder_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, ys = get_combined_normalized_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
