{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from read_scripts import dict_2_arr \n",
    "from read_scripts import read_dataset \n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "task = \"remote_homology\"\n",
    "\n",
    "def fit_logistic(X, y):\n",
    "    start = timer()\n",
    "    clf = LogisticRegression(max_iter=5000)\n",
    "    clf.fit(X, y)\n",
    "    end = timer()\n",
    "    print(f\"fit time: \", timedelta(seconds=end-start))\n",
    "    \n",
    "    train_score = clf.score(X, y)\n",
    "    print(f\"model train score: \", train_score)\n",
    "    \n",
    "    # when run will play a ping sound!\n",
    "    os.system(\"printf '\\a'\")\n",
    "    os.system(\"printf '\\a'\")\n",
    "    os.system(\"printf '\\a'\")\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_model_normalized_corpus(model):\n",
    "    ys = []\n",
    "    Xs = []\n",
    "\n",
    "    # read all splits\n",
    "    for split in ['train', 'valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']:\n",
    "\n",
    "        y_dict = read_dataset('label', task, split)\n",
    "        X, y = dict_2_arr(read_dataset(model, task, split), y_dict)\n",
    "        \n",
    "        ys.append(y)\n",
    "        Xs.append(X)  \n",
    "    \n",
    "    # concat all splits\n",
    "    corpus = np.concatenate(Xs, axis=0)\n",
    "    \n",
    "    return corpus, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_normalized_corpus():\n",
    "    ys = []\n",
    "    els = []\n",
    "    trfs = []\n",
    "    unis = []\n",
    "\n",
    "    # read all splits\n",
    "    for split in ['train', 'valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']:\n",
    "\n",
    "        y_dict = read_dataset('label', task, split)\n",
    "        X_e, y = dict_2_arr(read_dataset('elmo', task, split), y_dict)\n",
    "        X_t, y = dict_2_arr(read_dataset('transformer', task, split), y_dict)\n",
    "        X_u, y = dict_2_arr(read_dataset('unirep', task, split), y_dict)\n",
    "\n",
    "        ys.append(y)\n",
    "        els.append(X_e)  \n",
    "        trfs.append(X_t)\n",
    "        unis.append(X_u)\n",
    "    \n",
    "    # concat all splits\n",
    "    e_corpus = np.concatenate(els, axis=0)\n",
    "    t_corpus = np.concatenate(trfs, axis=0)\n",
    "    u_corpus = np.concatenate(unis, axis=0)\n",
    "    \n",
    "    # normalize each indvidually \n",
    "    e_corpus = preprocessing.normalize(e_corpus, norm='l2')\n",
    "    t_corpus = preprocessing.normalize(t_corpus, norm='l2')\n",
    "    u_corpus = preprocessing.normalize(u_corpus, norm='l2')\n",
    "    \n",
    "    # concatenate all corpuses\n",
    "    combined = np.concatenate([e_corpus, t_corpus, u_corpus], axis=1)\n",
    "    \n",
    "    return combined, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_over_corpus(corpus, ys, model_name):\n",
    "\n",
    "    all_score = {}\n",
    "\n",
    "    # get out training slice\n",
    "    X_train = corpus[:len(ys[0])]\n",
    "    y_train = ys[0]\n",
    "\n",
    "    # scale\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "\n",
    "    # fit!\n",
    "    clf = fit_logistic(X_train, y_train)\n",
    "\n",
    "    # record train score\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    print(f\"{model_name} train score: \", train_score)\n",
    "\n",
    "    all_score[\"train\"]  = train_score\n",
    "\n",
    "    # get slices for remaing splits and score\n",
    "    remain_splits = ['valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']\n",
    "    end = len(ys[0])\n",
    "    for i in range(len(remain_splits)):\n",
    "        split = remain_splits[i]\n",
    "        start = end\n",
    "        end = len(ys[i+1])+start\n",
    "\n",
    "        X = corpus[start:end]\n",
    "        y = ys[i+1]\n",
    "\n",
    "        X = scaler.transform(X)\n",
    "\n",
    "        test_score = clf.score(X, y)\n",
    "\n",
    "        all_score[split]  = test_score\n",
    "\n",
    "        print(f\"{model_name} {split} score: \", test_score)\n",
    "\n",
    "    return all_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_auto_encoder(X, latent_dim, hidden_layers):\n",
    "\n",
    "    n_input = X.shape[1]\n",
    "\n",
    "    full_layers = hidden_layers + [latent_dim] + hidden_layers[::-1]\n",
    "    print(\"layers\", full_layers)\n",
    "    \n",
    "    reg = MLPRegressor(hidden_layer_sizes = full_layers, \n",
    "                       activation = 'relu', \n",
    "                       solver = 'adam', \n",
    "                       learning_rate_init = 0.0001, \n",
    "                       max_iter = 100, \n",
    "                       tol = 0.0000001, \n",
    "                       verbose = False)\n",
    "    \n",
    "    print(\"fitting auto_encoder...\")\n",
    "    reg.fit(X, X)\n",
    "    \n",
    "    auto_encoder_train_score = reg.score(X, X)\n",
    "    print(\"auto_encoder_train_score:\", auto_encoder_train_score)\n",
    "    return reg\n",
    "\n",
    "\n",
    "def encoder(X, reg):\n",
    "    print(\"encoding...\")\n",
    "    X = np.asmatrix(X)\n",
    "    \n",
    "    encoder1 = X*reg.coefs_[0] + reg.intercepts_[0]\n",
    "    encoder1 = (np.exp(encoder1) - np.exp(-encoder1))/(np.exp(encoder1) + np.exp(-encoder1))\n",
    "    \n",
    "    encoder2 = encoder1*reg.coefs_[1] + reg.intercepts_[1]\n",
    "    encoder2 = (np.exp(encoder2) - np.exp(-encoder2))/(np.exp(encoder2) + np.exp(-encoder2))\n",
    "    \n",
    "    latent = encoder2*reg.coefs_[2] + reg.intercepts_[2]\n",
    "    latent = (np.exp(latent) - np.exp(-latent))/(np.exp(latent) + np.exp(-latent))\n",
    "    \n",
    "    print(\"encoded as: \", latent.shape)\n",
    "    \n",
    "    return np.asarray(latent)\n",
    "\n",
    "def train_and_encode(X, latent_dim=200, hidden_layers=[500, 300]):\n",
    "    \n",
    "    reg = train_auto_encoder(X, latent_dim, hidden_layers)\n",
    "    X_encoded = encoder(X, reg)\n",
    "    \n",
    "    return X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_e, ys = get_single_model_normalized_corpus('elmo')\n",
    "X_t, ys = get_single_model_normalized_corpus('transformer')\n",
    "X_u, ys = get_single_model_normalized_corpus('unirep')\n",
    "\n",
    "X_t_encoded = train_and_encode(X_t)\n",
    "X_u_encoded = train_and_encode(X_u)\n",
    "X_e_encoded = train_and_encode(X_e)\n",
    "\n",
    "conact_encoded = np.concatenate([X_t_encoded, X_u_encoded, X_e_encoded], axis=1)\n",
    "\n",
    "scores = train_test_over_corpus(conact_encoded, ys, \"auto_encode_then_concat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16281, 1900)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_conact_then_auto_encode_for_dim(X, ys, dim):\n",
    "    \n",
    "    hidden_layers = [1200, int((d+1200)/2)]\n",
    "    X = train_and_encode(X, latent_dim=dim, hidden_layers=hidden_layers)\n",
    "    scores = train_test_over_corpus(X, ys, f\"concat_then_auto_encode dim {dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16281, 3692)\n"
     ]
    }
   ],
   "source": [
    "X, ys = get_combined_normalized_corpus()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers [1200, 625, 50, 625, 1200]\n",
      "fitting auto_encoder...\n",
      "auto_encoder_train_score: 0.6232861587064792\n",
      "encoding...\n",
      "encoded as:  (16281, 50)\n"
     ]
    }
   ],
   "source": [
    "dims = []\n",
    "max_dim = 800\n",
    "for d in range(50, max_dim+50, 50):\n",
    "    dims.append(d)\n",
    "all_dim_all_scores = {}\n",
    "\n",
    "for d in dims:\n",
    "    s = do_conact_then_auto_encode_for_dim(X, ys, d)\n",
    "    all_dim_all_scores[str(d)] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"concat_then_auto_encode_across_dims.p\", \"wb\") as f:\n",
    "    pickle.dump(all_dim_all_scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
