{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', stream=sys.stdout)\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def print(self, msg):\n",
    "        print(f'{datetime.now().strftime(\"%H:%M:%S\")} {msg}')\n",
    "        \n",
    "L = Log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(model: str, task: str, split: str,  basepath='/scratch/fl1092/ml_protein_data'):\n",
    "    if model == 'transformer' and split == 'test': split='test_fold_holdout'\n",
    "    path_to_file = f'{basepath}/{model}/{task}/{task}_{split}.p'\n",
    "    data = np.load(path_to_file, allow_pickle=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def dict_2_arr(data_dict, labels, avgr=lambda x: np.mean(x, axis=0)):\n",
    "    \n",
    "    emb_shape = list(data_dict.values())[0].shape\n",
    "    number_of_embeddings = len(data_dict) \n",
    "\n",
    "    X = np.zeros((number_of_embeddings, emb_shape[-1]))\n",
    "    y = np.zeros(number_of_embeddings)\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    # iter over sorted keys in labels to ensure proteins\n",
    "    # from different models are indexed the same\n",
    "    keys = list(labels.keys())\n",
    "    keys.sort()\n",
    "    for key in keys :\n",
    "        if key in data_dict:\n",
    "            X[i] = avgr(data_dict[key])\n",
    "            y[i] = labels[key]\n",
    "            i += 1\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "def ensemble_append_mean_reps(dicts, labels, LEN, average=True, normalize=True):\n",
    "    # if average set to False, output 2d arrays for each sequence without averaging\n",
    "    if LEN == -1:\n",
    "        LEN = float('inf')\n",
    "        \n",
    "    keys = set(dicts[0].keys())\n",
    "    for i in range(1, len(dicts)):\n",
    "        keys = keys.intersection(set(dicts[i].keys()))\n",
    "    \n",
    "    # combine two dictionary into one by concatenating\n",
    "    new_dict = dict()\n",
    "    for key in keys:\n",
    "        seqs = []\n",
    "        for d in dicts:\n",
    "            \n",
    "            # 1d or 2d\n",
    "            if average:\n",
    "                seq = np.mean(d[key], axis=0)\n",
    "            else:\n",
    "                seq = d[key]\n",
    "                if seq.shape[0] < LEN:\n",
    "                    seq = np.concatenate([seq, np.zeros((LEN-seq.shape[0], seq.shape[1]))], axis=0)\n",
    "                elif seq.shape[0] > LEN:\n",
    "                    seq = seq[:LEN, ]\n",
    "                \n",
    "            # normalize or not\n",
    "            if normalize:\n",
    "                seq = preprocessing.normalize(seq.reshape(1, -1), norm='l2')\n",
    "\n",
    "            seqs.append(seq)\n",
    "        combined_seqs = np.concatenate(seqs, axis=-1)\n",
    "        \n",
    "        if average:\n",
    "            combined_seqs[0]\n",
    "        \n",
    "        new_dict[key] = combined_seqs\n",
    "    \n",
    "    emb_size = combined_seqs.shape\n",
    "    \n",
    "    if average:\n",
    "        X = np.zeros((len(new_dict), emb_size[1]))\n",
    "    else:\n",
    "        d1, d2 = emb_size\n",
    "        X = np.zeros((len(new_dict), d1, d2))\n",
    "    y = np.zeros(len(new_dict))\n",
    "    \n",
    "    i = 0\n",
    "    for key in new_dict:\n",
    "        X[i] = new_dict[key]\n",
    "        y[i] = labels[key]\n",
    "        i += 1\n",
    "        \n",
    "    print('concatenated embedding size: ', X.shape)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(T.utils.data.Dataset):\n",
    "    ### a Map-style dataset ###\n",
    "    \n",
    "    def __init__(self, task, split, average=True, normalize=True, conv=True, seq_len=-1):\n",
    "        self.models = ['elmo','unirep', 'transformer'] \n",
    "        self.splits = ['train', 'test']\n",
    "        self.x_data = {}\n",
    "        self.y_data = {}\n",
    "        self.LEN = seq_len\n",
    "        self.split = split # the split of the current data\n",
    "        \n",
    "        # for split in self.splits:\n",
    "        # load y\n",
    "        y_data = read_dataset('label', task, self.split)\n",
    "\n",
    "        # load x\n",
    "        to_append = []\n",
    "        for model in self.models:\n",
    "            data = read_dataset(model, task, self.split)\n",
    "            to_append.append(data)\n",
    "\n",
    "        # concatnate\n",
    "        X_app, self.y_data[split] = ensemble_append_mean_reps(to_append, y_data, self.LEN, average, normalize)\n",
    "        if conv:\n",
    "            # add channel information if convolutional neural net\n",
    "            X_app = X_app.reshape((X_app.shape[0], 1, X_app.shape[1], X_app.shape[2]))\n",
    "        print('Shape after appending: ', X_app.shape)\n",
    "        \n",
    "        self.x_data[self.split] = T.tensor(X_app, dtype=T.float32)\n",
    "            \n",
    "    def get_split(self, split):\n",
    "        assert(split in self.splits)\n",
    "        self.split = split\n",
    "        \n",
    "    def lenLongestSeq(self, task):\n",
    "        # find the length of the longest sequence\n",
    "        # TODO: trim and pad everyting to 95% interval\n",
    "        LEN = -1\n",
    "        for split in self.splits:\n",
    "            for model in self.models:\n",
    "                data = read_dataset(model, task, split)\n",
    "                \n",
    "                length = -1\n",
    "                for key, embedding in data.items():\n",
    "                    length = max(length, embedding.shape[0])\n",
    "                \n",
    "                print(f\"Longest sequence in {model} {split}: {length}\")\n",
    "                    \n",
    "                LEN = max(LEN, length)\n",
    "        \n",
    "        return LEN\n",
    "    \n",
    "    def lenRangeSeq(self, task, p=0.05):\n",
    "        # return the uper p-th percentile\n",
    "        lens = []\n",
    "        \n",
    "        for split in self.splits:\n",
    "            for model in self.models:\n",
    "                data = read_dataset(model, task, split)\n",
    "                \n",
    "                for key, embedding in data.items():\n",
    "                    lens.append(embedding.shape[0])\n",
    "                    \n",
    "        return \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data[self.split])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if T.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        x = self.x_data[self.split][idx, ]\n",
    "        y = self.y_data[self.split][idx]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulding blocks for autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### encoder and decoder with two convolutional layers ###\n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, c=10, latent_dims=512):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=c, kernel_size=4, stride=8, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=8, padding=1)\n",
    "        self.fc = nn.Linear(in_features=c*2*4*58, out_features=latent_dims)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #print(list(x.size()))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(list(x.size()))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(list(x.size()))\n",
    "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
    "        #print(list(x.size()))\n",
    "        x = self.fc(x)\n",
    "        #print(list(x.size()))\n",
    "        return x\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, c=10, latent_dims=512):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        self.c = c\n",
    "        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*4*58)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=8, padding=1)\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=8, padding=1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #print(list(x.size()))\n",
    "        x = self.fc(x)\n",
    "        #print(list(x.size()))\n",
    "        # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
    "        x = x.view(x.size(0), self.c*2, 4, 58)\n",
    "        #print(list(x.size()))\n",
    "        x = F.relu(self.conv2(x, output_size=[128, 10, 32, 462]))\n",
    "        #print(list(x.size()))\n",
    "        # last layer before output is tanh, since the images are normalized and 0-centered\n",
    "        x = T.tanh(self.conv1(x, output_size=[128, 1, 256, 3692]))\n",
    "        #print(list(x.size()))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### encoder and decoder with fully connected layers only ###\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_shape, out_features=600)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=128)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Decoder, self).__init__()\n",
    "        #self.fc1 = nn.Linear(in_features=128, out_features=128)\n",
    "        #self.fc2 = nn.Linear(in_features=128, out_features=1024)\n",
    "        self.fc3 = nn.Linear(in_features=600, out_features=input_shape)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_shape, conv=False, latent_dims=512):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        if conv:\n",
    "            L.print('Convolutional neural network')\n",
    "            self.encoder = ConvEncoder(latent_dims=latent_dims)\n",
    "            self.decoder = ConvDecoder(latent_dims=latent_dims)\n",
    "        else:\n",
    "            L.print('Fully connected neural network')\n",
    "            self.encoder = Encoder(input_shape)\n",
    "            self.decoder = Decoder(input_shape)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        x_recon = self.decoder(latent)\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loss functions ###\n",
    "\n",
    "def caemeLoss(outputs, batch_features, latent, d1, d2, l1=1, l2=1, l3=1):\n",
    "    mse = nn.MSELoss()\n",
    "    o1 = outputs[:, 0:d1]\n",
    "    o2 = outputs[:, d1:d1+d2]\n",
    "    f1 = batch_features[:, 0:d1]\n",
    "    f2 = batch_features[:, d1:d1+d2] \n",
    "    \n",
    "    return l1*mse(o1, f1) + l2*mse(o2, f2)\n",
    "\n",
    "def mseLoss(outputs, batch_features, latent, d1, d2, l1=1, l2=1, l3=1):\n",
    "    # mean-squared error loss\n",
    "    mse = nn.MSELoss() \n",
    "    \n",
    "    return mse(outputs, batch_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def metaEmbedding(data, model, device):\n",
    "#     X_latent = []\n",
    "#     Y = []\n",
    "\n",
    "#     for i in range(len(data)):\n",
    "#         x, y = data[i]\n",
    "#         X_latent.append(model.encoder(x.to(device)).cpu().detach().numpy())\n",
    "#         Y.append(y)\n",
    "        \n",
    "#     return X_latent, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metaEmbedding(data, model, device, conv=False):\n",
    "    X_latent = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        x, y = data[i]\n",
    "        # print(x.shape)\n",
    "        \n",
    "        if conv:\n",
    "            x = x.reshape(1, x.size()[0], x.size()[1], x.size()[2])\n",
    "        else:\n",
    "            x = x.reshape(1, x.size()[0])\n",
    "        # print(x.shape)\n",
    "            \n",
    "        x = x.to(device)\n",
    "        X_latent.append(model.encoder(x).cpu().detach().numpy()[0])\n",
    "        Y.append(y)\n",
    "        \n",
    "    return X_latent, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pipeline for training meta embedding and classifying ###\n",
    "\n",
    "def autoencoderPipeline(task, conv=True, normalize=True, latent_dimension=128, seq_len=-1,\n",
    "                        num_epochs=30, learning_rate=1e-3, batch_size=128, loss_function=mseLoss):\n",
    "    # if seq len is set to -1, then don't trim when doing cnn\n",
    "    \n",
    "    device = T.device('cuda')\n",
    "    if conv:\n",
    "        average=False\n",
    "    else:\n",
    "        average=True\n",
    "        \n",
    "    if conv:\n",
    "        # TODO: remove this\n",
    "        assert(normalize==False)\n",
    "    \n",
    "    L.print('Loading data ...')\n",
    "    train_data = DataSet(task, 'train', average, normalize, conv, seq_len)\n",
    "    test_data = DataSet(task, 'test', average, normalize, conv, seq_len)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = Autoencoder(3692, conv = conv, latent_dims=latent_dimension).to(device)\n",
    "    optimizer = T.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-7)\n",
    "    \n",
    "    L.print('Training autoencoder...')\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        loss = 0\n",
    "        for batch_features, _ in train_dataloader:\n",
    "            \n",
    "            batch_features = batch_features.to(device) # load data to GPU\n",
    "            optimizer.zero_grad() # reset the gradients back to zero\n",
    "            \n",
    "            outputs = model(batch_features) # compute reconstructions\n",
    "            latent = model.encoder(batch_features) # compute latent meta embedding\n",
    "            \n",
    "            train_loss = loss_function(outputs, batch_features, latent, 1024, 1900) # compute training loss\n",
    "            train_loss.backward() # compute accumulated gradients\n",
    "            optimizer.step() # perform parameter update based on current gradients\n",
    "            loss += train_loss.item() # add the mini-batch training loss to epoch loss\n",
    "\n",
    "        loss = loss / len(train_dataloader) # compute the epoch training loss\n",
    "\n",
    "        if epoch %3 == 0:\n",
    "            L.print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, num_epochs, loss))\n",
    "            \n",
    "    L.print('Training classifier ...')\n",
    "    \n",
    "    test_x_latent, test_y = metaEmbedding(test_data, model, device, conv)\n",
    "    train_x_latent, train_y = metaEmbedding(train_data, model, device, conv)\n",
    "    \n",
    "    clf = make_pipeline(StandardScaler(), LogisticRegression(solver='liblinear', multi_class='auto'))\n",
    "    clf.fit(train_x_latent, train_y)\n",
    "    \n",
    "    score = clf.score(test_x_latent, test_y)\n",
    "    \n",
    "    L.print(f\"Score on test set: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04:09:04 Loading data ...\n",
      "concatenated embedding size:  (12305, 256, 3692)\n",
      "Shape after appending:  (12305, 1, 256, 3692)\n",
      "concatenated embedding size:  (718, 256, 3692)\n",
      "Shape after appending:  (718, 1, 256, 3692)\n",
      "CPU times: user 1min 41s, sys: 2min 53s, total: 4min 34s\n",
      "Wall time: 4min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "task='remote_homology'\n",
    "conv=True\n",
    "normalize=False\n",
    "latent_dimension=3692\n",
    "seq_len=256\n",
    "num_epochs=30\n",
    "learning_rate=1e-3\n",
    "batch_size=128\n",
    "loss_function=mseLoss\n",
    "\n",
    "device = T.device('cuda')\n",
    "if conv:\n",
    "    average=False\n",
    "else:\n",
    "    average=True\n",
    "\n",
    "if conv:\n",
    "    # TODO: remove this\n",
    "    assert(normalize==False)\n",
    "\n",
    "L.print('Loading data ...')\n",
    "train_data = DataSet(task, 'train', average, normalize, conv, seq_len)\n",
    "test_data = DataSet(task, 'test', average, normalize, conv, seq_len)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04:14:52 Convolutional neural network\n",
      "04:14:54 Training autoencoder...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3f81e927ae454789e63812977d4850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04:15:20 epoch : 1/30, loss = 0.129377\n",
      "04:16:27 epoch : 4/30, loss = 0.122814\n",
      "04:17:32 epoch : 7/30, loss = 0.122956\n",
      "04:18:37 epoch : 10/30, loss = 0.122851\n",
      "04:19:42 epoch : 13/30, loss = 0.123096\n",
      "04:20:48 epoch : 16/30, loss = 0.122819\n",
      "04:21:53 epoch : 19/30, loss = 0.123087\n",
      "04:22:58 epoch : 22/30, loss = 0.122445\n",
      "04:24:02 epoch : 25/30, loss = 0.122827\n",
      "04:25:07 epoch : 28/30, loss = 0.122698\n",
      "04:26:13 epoch : 31/30, loss = 0.122766\n",
      "04:27:18 epoch : 34/30, loss = 0.122578\n",
      "04:28:23 epoch : 37/30, loss = 0.122420\n",
      "04:29:28 epoch : 40/30, loss = 0.122365\n",
      "04:30:33 epoch : 43/30, loss = 0.122931\n",
      "04:31:38 epoch : 46/30, loss = 0.122622\n",
      "04:32:43 epoch : 49/30, loss = 0.122410\n",
      "04:33:48 epoch : 52/30, loss = 0.122411\n",
      "04:34:53 epoch : 55/30, loss = 0.123193\n",
      "04:35:58 epoch : 58/30, loss = 0.122592\n",
      "04:37:03 epoch : 61/30, loss = 0.122622\n",
      "04:38:09 epoch : 64/30, loss = 0.122750\n",
      "04:39:14 epoch : 67/30, loss = 0.122464\n",
      "04:40:19 epoch : 70/30, loss = 0.124100\n",
      "04:41:24 epoch : 73/30, loss = 0.123056\n",
      "04:42:29 epoch : 76/30, loss = 0.122913\n",
      "04:43:34 epoch : 79/30, loss = 0.122685\n",
      "04:44:39 epoch : 82/30, loss = 0.122563\n",
      "04:45:44 epoch : 85/30, loss = 0.123044\n",
      "04:46:49 epoch : 88/30, loss = 0.122781\n",
      "04:47:54 epoch : 91/30, loss = 0.122580\n",
      "04:48:59 epoch : 94/30, loss = 0.124106\n",
      "04:50:05 epoch : 97/30, loss = 0.122993\n",
      "04:51:10 epoch : 100/30, loss = 0.122736\n",
      "\n",
      "CPU times: user 40min 4s, sys: 9min 33s, total: 49min 38s\n",
      "Wall time: 36min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Autoencoder(3692, conv = conv, latent_dims=latent_dimension).to(device)\n",
    "optimizer = T.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-7)\n",
    "\n",
    "L.print('Training autoencoder...')\n",
    "for epoch in tqdm(range(100)):\n",
    "    loss = 0\n",
    "    for batch_features, _ in train_dataloader:\n",
    "\n",
    "        batch_features = batch_features.to(device) # load data to GPU\n",
    "        optimizer.zero_grad() # reset the gradients back to zero\n",
    "\n",
    "        outputs = model(batch_features) # compute reconstructions\n",
    "        latent = model.encoder(batch_features) # compute latent meta embedding\n",
    "\n",
    "        train_loss = loss_function(outputs, batch_features, latent, 1024, 1900) # compute training loss\n",
    "        train_loss.backward() # compute accumulated gradients\n",
    "        optimizer.step() # perform parameter update based on current gradients\n",
    "        loss += train_loss.item() # add the mini-batch training loss to epoch loss\n",
    "\n",
    "    loss = loss / len(train_dataloader) # compute the epoch training loss\n",
    "\n",
    "    if epoch %3 == 0:\n",
    "        L.print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, num_epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04:51:10 Training classifier ...\n",
      "CPU times: user 18.7 s, sys: 5.42 s, total: 24.1 s\n",
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "L.print('Training classifier ...')\n",
    "    \n",
    "test_x_latent, test_y = metaEmbedding(test_data, model, device, conv)\n",
    "train_x_latent, train_y = metaEmbedding(train_data, model, device, conv)\n",
    "\n",
    "with open(f'train_{conv}_{latent_dimension}.npy', 'wb') as f:\n",
    "    np.save(f, np.array(train_x_latent))\n",
    "\n",
    "with open(f'test_{conv}_{latent_dimension}.npy', 'wb') as f:\n",
    "    np.save(f, np.array(test_x_latent))\n",
    "    \n",
    "with open(f'train_{conv}_{latent_dimension}_y.npy', 'wb') as f:\n",
    "    np.save(f, np.array(train_y))\n",
    "    \n",
    "with open(f'test_{conv}_{latent_dimension}_y.npy', 'wb') as f:\n",
    "        np.save(f, np.array(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(StandardScaler(), LogisticRegression(solver='liblinear', multi_class='auto'))\n",
    "clf.fit(train_x_latent, train_y)\n",
    "\n",
    "score = clf.score(test_x_latent, test_y)\n",
    "\n",
    "L.print(f\"Score on test set: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Shape after appending:  (12305, 1, 256, 3692)\n"
     ]
    }
   ],
   "source": [
    "task = 'remote_homology'\n",
    "batch_size = 128\n",
    "\n",
    "device = T.device('cuda')\n",
    "\n",
    "print('Loading data ...')\n",
    "\n",
    "train_data = DataSet(task, 'train', average=False, normalize=False)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12305"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training autoencoder...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98337a66b8aa4c309b060d325c2b2ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/30, loss = 0.131212\n",
      "epoch : 4/30, loss = 0.094607\n",
      "epoch : 7/30, loss = 0.087955\n",
      "epoch : 10/30, loss = 0.085494\n",
      "epoch : 13/30, loss = 0.084361\n",
      "epoch : 16/30, loss = 0.083872\n",
      "epoch : 19/30, loss = 0.083340\n",
      "epoch : 22/30, loss = 0.083148\n",
      "epoch : 25/30, loss = 0.082889\n",
      "epoch : 28/30, loss = 0.082713\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 30\n",
    "model = Autoencoder(2924, conv=True).to(device)\n",
    "optimizer = T.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-7)\n",
    "loss_function = mseLoss\n",
    "\n",
    "print('Training autoencoder...')\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    loss = 0\n",
    "    for batch_features, _ in train_dataloader:\n",
    "\n",
    "        batch_features = batch_features.to(device) # load data to GPU\n",
    "        optimizer.zero_grad() # reset the gradients back to zero\n",
    "\n",
    "        outputs = model(batch_features) # compute reconstructions\n",
    "        latent = model.encoder(batch_features) # compute latent meta embedding\n",
    "        # print(list(outputs.size()), list(batch_features.size()))\n",
    "        \n",
    "        train_loss = loss_function(outputs, batch_features, latent, 1024, 1900) # compute training loss\n",
    "        train_loss.backward() # compute accumulated gradients\n",
    "        optimizer.step() # perform parameter update based on current gradients\n",
    "        loss += train_loss.item() # add the mini-batch training loss to epoch loss\n",
    "\n",
    "    loss = loss / len(train_dataloader) # compute the epoch training loss\n",
    "\n",
    "    if epoch %3 == 0:\n",
    "        print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, num_epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after appending:  (718, 1, 256, 3692)\n"
     ]
    }
   ],
   "source": [
    "test_data = DataSet(task, 'test', average=False, normalize=False)\n",
    "test_data.get_split('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metaEmbedding(data, model):\n",
    "    X_latent = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        x, y = data[i]\n",
    "        x = x.reshape(1, x.size()[0], x.size()[1], x.size()[2]).to(device)\n",
    "        X_latent.append(model.encoder(x).cpu().detach().numpy()[0])\n",
    "        Y.append(y)\n",
    "        \n",
    "    return X_latent, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 4.66 s, total: 24 s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_x_latent, test_y = metaEmbedding(test_data, model)\n",
    "train_x_latent, train_y = metaEmbedding(train_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 37s, sys: 1min 1s, total: 49min 38s\n",
      "Wall time: 49min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression(solver='liblinear', multi_class='auto'))\n",
    "clf.fit(train_x_latent, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.score(test_x_latent, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25348189415041783"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score # 0.2479108635097493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 30\n",
    "model = Autoencoder(2924, conv=True, latent_dims=2048).to(device)\n",
    "optimizer = T.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-7)\n",
    "loss_function = mseLoss\n",
    "\n",
    "print('Training autoencoder...')\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    loss = 0\n",
    "    for batch_features, _ in train_dataloader:\n",
    "\n",
    "        batch_features = batch_features.to(device) # load data to GPU\n",
    "        optimizer.zero_grad() # reset the gradients back to zero\n",
    "\n",
    "        outputs = model(batch_features) # compute reconstructions\n",
    "        latent = model.encoder(batch_features) # compute latent meta embedding\n",
    "        # print(list(outputs.size()), list(batch_features.size()))\n",
    "        \n",
    "        train_loss = loss_function(outputs, batch_features, latent, 1024, 1900) # compute training loss\n",
    "        train_loss.backward() # compute accumulated gradients\n",
    "        optimizer.step() # perform parameter update based on current gradients\n",
    "        loss += train_loss.item() # add the mini-batch training loss to epoch loss\n",
    "\n",
    "    loss = loss / len(train_dataloader) # compute the epoch training loss\n",
    "\n",
    "    if epoch %3 == 0:\n",
    "        print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, num_epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2479108635097493"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fl1092/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), LogisticRegression(solver='liblinear'))\n",
    "clf.fit(train_x_latent, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.score(test_x_latent, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2479108635097493"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Training autoencoder...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f695e07470d428089c9607e793f2730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/30, loss = 0.000525\n",
      "epoch : 4/30, loss = 0.000464\n",
      "epoch : 7/30, loss = 0.000450\n",
      "epoch : 10/30, loss = 0.000443\n",
      "epoch : 13/30, loss = 0.000440\n",
      "epoch : 16/30, loss = 0.000439\n",
      "epoch : 19/30, loss = 0.000438\n",
      "epoch : 22/30, loss = 0.000437\n",
      "epoch : 25/30, loss = 0.000437\n",
      "epoch : 28/30, loss = 0.000437\n",
      "\n",
      "Training classifier ...\n",
      "Score on test set: 0.25487465181058494\n",
      "CPU times: user 3min 50s, sys: 35.7 s, total: 4min 26s\n",
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "autoencoderPipeline('remote_homology', loss_function=mseLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Training autoencoder...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54358dc23516430cab40bfaec8c7d3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/30, loss = 0.001178\n",
      "epoch : 4/30, loss = 0.001067\n",
      "epoch : 7/30, loss = 0.001046\n",
      "epoch : 10/30, loss = 0.001031\n",
      "epoch : 13/30, loss = 0.001025\n",
      "epoch : 16/30, loss = 0.001021\n",
      "epoch : 19/30, loss = 0.001018\n",
      "epoch : 22/30, loss = 0.001015\n",
      "epoch : 25/30, loss = 0.001014\n",
      "epoch : 28/30, loss = 0.001013\n",
      "\n",
      "Training classifier ...\n",
      "Score on test set: 0.2646239554317549\n",
      "CPU times: user 3min 49s, sys: 36 s, total: 4min 25s\n",
      "Wall time: 4min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "autoencoderPipeline('remote_homology', loss_function=caemeLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
