{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model of AEME\n",
    "# File: model.py\n",
    "# Author: Cong Bao\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as skpre\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "\n",
    "__author__ = 'Cong Bao'\n",
    "\n",
    "class AEME(object):\n",
    "    \"\"\" Autoencoded Meta-Embedding.\n",
    "        :param input_list: a list of source embedding path\n",
    "        :param output_path: a string path of output file\n",
    "        :param log_path: a string path of log file\n",
    "        :param ckpt_path: a string path of checkpoint file\n",
    "        :param model_type: the type of model, among DAEME, CAEME, AAEME\n",
    "        :param dims: a list of dimensionalities of each source embedding\n",
    "        :param learning_rate: a float number of the learning rate\n",
    "        :param batch_size: a int number of the batch size\n",
    "        :param epoch: a int number of the epoch\n",
    "        :param activ: a string name of activation function\n",
    "        :param factors: a list of coefficients of each loss part\n",
    "        :param noise: a float number between 0 and 1 of the masking noise rate\n",
    "        :param emb_dim: a int number of meta-embedding dimensionality, only used in AAEME model\n",
    "        :param oov: a boolean value whether to initialize inputs with oov or not\n",
    "        :param restore: a boolean value whether to restore checkpoint from local file or \n",
    "        :property logger: a logger to record log information\n",
    "        :property utils: a utility tool for I/O\n",
    "        :property sess: a tensorflow session\n",
    "        :property ckpt: a tensorflow saver \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model_type = kwargs['model']\n",
    "        self.dims = kwargs['dims']\n",
    "        self.learning_rate = kwargs['learning_rate']\n",
    "        self.batch_size = kwargs['batch']\n",
    "        self.epoch = kwargs['epoch']\n",
    "        self.activ = kwargs['activ']\n",
    "        self.factors = kwargs['factors']\n",
    "        self.noise = kwargs['noise']\n",
    "        self.emb_dim = kwargs['emb']\n",
    "        self.oov = kwargs['oov']\n",
    "        self.ckpt_path = kwargs['ckpt'] + 'model.ckpt'\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.ckpt = None\n",
    "\n",
    "    def load_data(self, src_dict_list):\n",
    "        \"\"\" Load individual source embeddings and store intersection/union words and embeddings in separate lists.\n",
    "            If oov is True, word list will be the union of individual vocabularies.\n",
    "            If oov is False, word list will be the intersection of individual vocabularies.\n",
    "        \"\"\"\n",
    "\n",
    "        self.inter_words = list(set.intersection(*[set(src_dict.keys()) for src_dict in src_dict_list]))\n",
    "        print('Intersection Words: %s' % len(self.inter_words))\n",
    "        self.sources = np.asarray(list(zip(*[skpre.normalize([src_dict[word] for word in self.inter_words]) for src_dict in src_dict_list])))\n",
    "       \n",
    "        # delete the list of dicts to release memory\n",
    "        del src_dict_list\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\" Build the model of AEME.\n",
    "            The model to build will be one of DAEME, CAEME, or AAEME.\n",
    "        \"\"\"\n",
    "        # initialize sources and inputs\n",
    "        self.srcs = [tf.placeholder(tf.float32, (None, dim)) for dim in self.dims]\n",
    "        self.ipts = [tf.placeholder(tf.float32, (None, dim)) for dim in self.dims]\n",
    "        # select and build models\n",
    "        params = [self.dims, self.activ, self.factors]\n",
    "        if self.model_type == 'DAEME':\n",
    "            self.aeme = DAEME(*params)\n",
    "        elif self.model_type == 'CAEME':\n",
    "            self.aeme = CAEME(*params)\n",
    "        elif self.model_type == 'AAEME':\n",
    "            self.aeme = AAEME(*params, emb_dim=self.emb_dim)\n",
    "        self.aeme.build(self.srcs, self.ipts)\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\" Train the model.\n",
    "            Variables with least losses will be stored in checkpoint file.\n",
    "        \"\"\"\n",
    "        step = tf.Variable(0, trainable=False)\n",
    "        rate = tf.train.exponential_decay(self.learning_rate, step, 50, 0.99)\n",
    "        loss = self.aeme.loss()\n",
    "        opti = tf.train.AdamOptimizer(rate).minimize(loss, global_step=step)\n",
    "        self.ckpt = tf.train.Saver(tf.global_variables())\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        size = len(self.sources) // self.batch_size # the number of batches\n",
    "        best = float('inf')\n",
    "        # loop for N epoches\n",
    "        for itr in range(self.epoch):\n",
    "            indexes = np.random.permutation(len(self.sources)) # shuffle training inputs\n",
    "            train_loss = 0.\n",
    "            # train with mini-batches\n",
    "            for idx in range(size):\n",
    "                batch_idx = indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "                batches = list(zip(*self.sources[batch_idx]))\n",
    "                feed = {k:v for k, v in zip(self.srcs, batches)}\n",
    "                feed.update({k:self._corrupt(v) for k, v in zip(self.ipts, batches)})\n",
    "                _, batch_loss = self.sess.run([opti, loss], feed)\n",
    "                train_loss += batch_loss\n",
    "            epoch_loss = train_loss / size\n",
    "            # save the checkpoint with least loss\n",
    "            if epoch_loss <= best:\n",
    "                self.ckpt.save(self.sess, self.ckpt_path)\n",
    "                best = epoch_loss\n",
    "            print('[Epoch{0}] loss: {1}'.format(itr, epoch_loss))\n",
    "\n",
    "    def generate_meta_embed(self):\n",
    "        \"\"\" Generate meta-embedding and save as local file.\n",
    "            Variables used to predict are these with least losses during training.\n",
    "        \"\"\"\n",
    "        embed= {}\n",
    "        print('Generating meta embeddings...')\n",
    "        self.ckpt.restore(self.sess, self.ckpt_path)\n",
    "        if self.oov:\n",
    "            vocabulary = self.union_words\n",
    "        else:\n",
    "            vocabulary = self.inter_words\n",
    "        for i, word in enumerate(vocabulary):\n",
    "            meta = self.sess.run(self.aeme.extract(), {k:[v] for k, v in zip(self.ipts, self.sources[i])})\n",
    "            embed[word] = np.reshape(meta, (np.shape(meta)[1],))\n",
    "        self.sess.close()\n",
    "        del self.sources\n",
    "        return embed\n",
    "\n",
    "    def _corrupt(self, batch):\n",
    "        \"\"\" Corrupt a batch using masking noises.\n",
    "            :param batch: the batch to be corrupted\n",
    "            :return: a new batch after corrupting\n",
    "        \"\"\"\n",
    "        noised = np.copy(batch)\n",
    "        batch_size, feature_size = np.shape(batch)\n",
    "        for i in range(batch_size):\n",
    "            mask = np.random.randint(0, feature_size, int(feature_size * self.noise))\n",
    "            for m in mask:\n",
    "                noised[i][m] = 0.\n",
    "        return noised\n",
    "\n",
    "class AbsModel(object):\n",
    "    \"\"\" Base class of all proposed methods.\n",
    "        :param dims: a list of dimensionalities of each input\n",
    "        :param activ: the string name of activation function\n",
    "        :param factors: a list of coefficients of each loss part\n",
    "    \"\"\"\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, dims, activ, factors):\n",
    "        self.dims = dims\n",
    "        self.factors = factors\n",
    "\n",
    "        if activ == 'lrelu':\n",
    "            self.activ = tf.keras.layers.LeakyReLU(0.2)\n",
    "        elif activ == 'prelu':\n",
    "            self.activ = tf.keras.layers.PReLU()\n",
    "        else:\n",
    "            self.activ = tf.keras.layers.Activation(activ)\n",
    "\n",
    "        self.meta = None\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(x, y, f):\n",
    "        \"\"\" Mean Squared Error with slicing.\n",
    "            This method will slice vector with higher dimension to the lower one,\n",
    "            if the two vector have different dimensions.\n",
    "            :param x: first vector\n",
    "            :param y: second vector\n",
    "            :param f: coefficient\n",
    "            :return: a tensor after calculating f * (1 / d) * ||x - y||^2\n",
    "        \"\"\"\n",
    "        x_d = x.get_shape().as_list()[1]\n",
    "        y_d = y.get_shape().as_list()[1]\n",
    "        if x_d != y_d:\n",
    "            smaller = min(x_d, y_d)\n",
    "            x = tf.slice(x, [0, 0], [tf.shape(x)[0], smaller])\n",
    "            y = tf.slice(y, [0, 0], [tf.shape(y)[0], smaller])\n",
    "        return tf.scalar_mul(f, tf.reduce_mean(tf.squared_difference(x, y)))\n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\" Extract the meta-embeddding model.\n",
    "            :return: the meta-embedding model\n",
    "        \"\"\"\n",
    "        return self.meta\n",
    "\n",
    "    @abstractmethod\n",
    "    def build(self, srcs, ipts):\n",
    "        \"\"\" Abstract method.\n",
    "            Build the model.\n",
    "            :param srcs: source embeddings\n",
    "            :param ipts: input embeddings\n",
    "        \"\"\"\n",
    "        self.srcs = srcs\n",
    "        self.ipts = ipts\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss(self):\n",
    "        \"\"\" Abstract method.\n",
    "            Obtain the loss function of model.\n",
    "            :return: a tensor calculating the loss function\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class DAEME(AbsModel):\n",
    "    \"\"\" Decoupled Autoencoded Meta-Embedding.\n",
    "        This method calculate meta-embedding as the concatenation of encoded source embeddings.\n",
    "        The loss function is defined as the sum of mse of each autoencoder and the mse between meta parts.\n",
    "    \"\"\"\n",
    "\n",
    "    def build(self, srcs, ipts):\n",
    "        AbsModel.build(self, srcs, ipts)\n",
    "        self.encoders = [tf.layers.dense(ipt, dim, self.activ) for ipt, dim in zip(self.ipts, self.dims)]\n",
    "        self.meta = tf.nn.l2_normalize(tf.concat(self.encoders, 1), 1)\n",
    "        self.outs = [tf.layers.dense(encoder, dim) for encoder, dim in zip(self.encoders, self.dims)]\n",
    "\n",
    "    def loss(self):\n",
    "        los = tf.add_n([self.mse(x, y, f) for x, y, f in zip(self.srcs, self.outs, self.factors[:-1])])\n",
    "        for x, y in combinations(self.encoders, 2):\n",
    "            los = tf.add(los, self.mse(x, y, self.factors[-1]))\n",
    "        return los\n",
    "\n",
    "class CAEME(AbsModel):\n",
    "    \"\"\" Concatenated Autoencoded Meta-Embedding.\n",
    "        This method calculate meta-embedding as the concatenation of encoded source embeddings.\n",
    "        The loss function is defined as the sum of mse of each autoencoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def build(self, srcs, ipts):\n",
    "        AbsModel.build(self, srcs, ipts)\n",
    "        self.encoders = [tf.layers.dense(ipt, dim, self.activ) for ipt, dim in zip(self.ipts, self.dims)]\n",
    "        self.meta = tf.nn.l2_normalize(tf.concat(self.encoders, 1), 1)\n",
    "        self.outs = [tf.layers.dense(self.meta, dim) for dim in self.dims]\n",
    "\n",
    "    def loss(self):\n",
    "        return tf.add_n([self.mse(x, y, f) for x, y, f in zip(self.srcs, self.outs, self.factors)])\n",
    "\n",
    "class AAEME(AbsModel):\n",
    "    \"\"\" Averaged Autoencoded Meta-Embedding.\n",
    "        This method calculate meta-embedding as the averaging of encoded source embeddings.\n",
    "        The loss function is defined as the sum of mse of each autoencoder.\n",
    "        :param emb_dim: the dimensionality of meta-embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        AbsModel.__init__(self, *args)\n",
    "        self.emb_dim = kwargs['emb_dim']\n",
    "\n",
    "    def build(self, srcs, ipts):\n",
    "        AbsModel.build(self, srcs, ipts)\n",
    "        self.encoders = [tf.layers.dense(ipt, self.emb_dim, self.activ) for ipt in self.ipts]\n",
    "        self.meta = tf.nn.l2_normalize(tf.add_n(self.encoders), 1)\n",
    "        self.outs = [tf.layers.dense(self.meta, dim) for dim in self.dims]\n",
    "\n",
    "    def loss(self):\n",
    "        return tf.add_n([self.mse(x, y, f) for x, y, f in zip(self.srcs, self.outs, self.factors)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_model_corpus_mean_dict(model):\n",
    "    \n",
    "    ds = []\n",
    "    # read all dicts\n",
    "    for split in ['train', 'valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']:\n",
    "\n",
    "        d = read_dataset(\"elmo\", \"remote_homology\", split)   \n",
    "        ds.append(d)\n",
    "\n",
    "    all_d = {}\n",
    "    \n",
    "    for d in ds:\n",
    "        for k in list(d.keys()):\n",
    "            seq = d[k]\n",
    "            all_d[k] = np.mean(seq, axis=0)\n",
    "\n",
    "    return all_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_scripts import read_dataset \n",
    "\n",
    "e_dict = get_single_model_corpus_mean_dict(\"elmo\")\n",
    "t_dict = get_single_model_corpus_mean_dict(\"transformer\")\n",
    "u_dict = get_single_model_corpus_mean_dict(\"unirep\")\n",
    "\n",
    "list_emb_dicts = [e_dict, t_dict, u_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = []\n",
    "for d in list_emb_dicts:\n",
    "    dims.append(list(d.values())[0].shape[0])\n",
    "    \n",
    "params = {}\n",
    "params['model'] = 'CAEME' # among DAEME, CAEME, AAEME\n",
    "params['emb'] = 200 # a int number of meta-embedding dimensionality, only used in AAEME model\n",
    "params['dims'] = dims # a list of encoded dimensionalities for each source embedding\n",
    "params['learning_rate'] = 0.001\n",
    "params['batch'] = 128\n",
    "params['epoch'] = 100\n",
    "params['activ'] = 'relu'\n",
    "params['factors'] = [1.0, 1.0, 1.0]\n",
    "params['noise'] = 0.05\n",
    "params['oov'] = False\n",
    "params['ckpt'] = './aeme'\n",
    "\n",
    "aeme = AEME(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection Words: 16281\n"
     ]
    }
   ],
   "source": [
    "aeme.load_data(list_emb_dicts)\n",
    "aeme.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch0] loss: 0.000775664323834541\n",
      "[Epoch1] loss: 0.00029202072762455207\n",
      "[Epoch2] loss: 0.0001894768764490379\n",
      "[Epoch3] loss: 0.00014432385166479993\n",
      "[Epoch4] loss: 0.00011947967719807341\n",
      "[Epoch5] loss: 0.00010351678500901614\n",
      "[Epoch6] loss: 9.282371538001898e-05\n",
      "[Epoch7] loss: 8.538343392974353e-05\n",
      "[Epoch8] loss: 7.939211738673515e-05\n",
      "[Epoch9] loss: 7.467825945620756e-05\n",
      "[Epoch10] loss: 7.117578132338382e-05\n",
      "[Epoch11] loss: 6.810573947626921e-05\n",
      "[Epoch12] loss: 6.561333601872663e-05\n",
      "[Epoch13] loss: 6.359878947062777e-05\n",
      "[Epoch14] loss: 6.151450401461705e-05\n",
      "[Epoch15] loss: 5.996403800906599e-05\n",
      "[Epoch16] loss: 5.842027937655161e-05\n",
      "[Epoch17] loss: 5.712991731157013e-05\n",
      "[Epoch18] loss: 5.604783907224695e-05\n",
      "[Epoch19] loss: 5.484162901430097e-05\n",
      "[Epoch20] loss: 5.386704213479094e-05\n",
      "[Epoch21] loss: 5.302411645827778e-05\n",
      "[Epoch22] loss: 5.216189634580143e-05\n",
      "[Epoch23] loss: 5.145931638654424e-05\n",
      "[Epoch24] loss: 5.070279027183181e-05\n",
      "[Epoch25] loss: 4.996561840489162e-05\n",
      "[Epoch26] loss: 4.943814548007469e-05\n",
      "[Epoch27] loss: 4.882103553606476e-05\n",
      "[Epoch28] loss: 4.8139720594173105e-05\n",
      "[Epoch29] loss: 4.770327153190325e-05\n",
      "[Epoch30] loss: 4.7155867553477386e-05\n",
      "[Epoch31] loss: 4.669904632550686e-05\n",
      "[Epoch32] loss: 4.621841412520689e-05\n",
      "[Epoch33] loss: 4.565897365041099e-05\n",
      "[Epoch34] loss: 4.534100553714743e-05\n",
      "[Epoch35] loss: 4.496461429150122e-05\n",
      "[Epoch36] loss: 4.456820502450074e-05\n",
      "[Epoch37] loss: 4.4221690493612e-05\n",
      "[Epoch38] loss: 4.3977924200246026e-05\n",
      "[Epoch39] loss: 4.353423069041464e-05\n",
      "[Epoch40] loss: 4.3305647315404295e-05\n",
      "[Epoch41] loss: 4.299469477213794e-05\n",
      "[Epoch42] loss: 4.2699780043601386e-05\n",
      "[Epoch43] loss: 4.2383290296072124e-05\n",
      "[Epoch44] loss: 4.210908408872563e-05\n",
      "[Epoch45] loss: 4.184052359480521e-05\n",
      "[Epoch46] loss: 4.154079547656084e-05\n",
      "[Epoch47] loss: 4.1423086609781216e-05\n",
      "[Epoch48] loss: 4.116166340059777e-05\n",
      "[Epoch49] loss: 4.086386702045446e-05\n",
      "[Epoch50] loss: 4.072889147553681e-05\n",
      "[Epoch51] loss: 4.052975218226092e-05\n",
      "[Epoch52] loss: 4.032056247123415e-05\n",
      "[Epoch53] loss: 4.013440245510387e-05\n",
      "[Epoch54] loss: 3.989379527368106e-05\n",
      "[Epoch55] loss: 3.9729227836236475e-05\n",
      "[Epoch56] loss: 3.9536165829526084e-05\n",
      "[Epoch57] loss: 3.940186205028564e-05\n",
      "[Epoch58] loss: 3.92042906972301e-05\n",
      "[Epoch59] loss: 3.911305431370059e-05\n",
      "[Epoch60] loss: 3.894270950810082e-05\n",
      "[Epoch61] loss: 3.88728287763065e-05\n",
      "[Epoch62] loss: 3.865598432818706e-05\n",
      "[Epoch63] loss: 3.844045170085087e-05\n",
      "[Epoch64] loss: 3.8380309838323125e-05\n",
      "[Epoch65] loss: 3.820444120591008e-05\n",
      "[Epoch66] loss: 3.8042446908011194e-05\n",
      "[Epoch67] loss: 3.7949729924398205e-05\n",
      "[Epoch68] loss: 3.785337642289874e-05\n",
      "[Epoch69] loss: 3.7710831246676116e-05\n",
      "[Epoch70] loss: 3.761272406751862e-05\n",
      "[Epoch71] loss: 3.758468788358251e-05\n",
      "[Epoch72] loss: 3.743096841196713e-05\n",
      "[Epoch73] loss: 3.7284734812207654e-05\n",
      "[Epoch74] loss: 3.7170359222025896e-05\n",
      "[Epoch75] loss: 3.707059306724136e-05\n",
      "[Epoch76] loss: 3.701306596617434e-05\n",
      "[Epoch77] loss: 3.6919678906968787e-05\n",
      "[Epoch78] loss: 3.6801296842236835e-05\n",
      "[Epoch79] loss: 3.667670947418907e-05\n",
      "[Epoch80] loss: 3.665911809928114e-05\n",
      "[Epoch81] loss: 3.660927375060814e-05\n",
      "[Epoch82] loss: 3.650171080568446e-05\n",
      "[Epoch83] loss: 3.63817222725314e-05\n",
      "[Epoch84] loss: 3.6314306314347136e-05\n",
      "[Epoch85] loss: 3.625715123242002e-05\n",
      "[Epoch86] loss: 3.61581907610365e-05\n",
      "[Epoch87] loss: 3.611827411278202e-05\n",
      "[Epoch88] loss: 3.600596629819231e-05\n",
      "[Epoch89] loss: 3.597542632547185e-05\n",
      "[Epoch90] loss: 3.5907016053253155e-05\n",
      "[Epoch91] loss: 3.5847947308537e-05\n",
      "[Epoch92] loss: 3.576205090498625e-05\n",
      "[Epoch93] loss: 3.567103927406871e-05\n",
      "[Epoch94] loss: 3.565982791139583e-05\n",
      "[Epoch95] loss: 3.554743355874331e-05\n",
      "[Epoch96] loss: 3.553375604747626e-05\n",
      "[Epoch97] loss: 3.544217858194946e-05\n",
      "[Epoch98] loss: 3.5413413742368336e-05\n",
      "[Epoch99] loss: 3.53725071628413e-05\n"
     ]
    }
   ],
   "source": [
    "aeme.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n"
     ]
    }
   ],
   "source": [
    "embeded_corpus = aeme.generate_meta_embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from read_scripts import dict_2_arr \n",
    "from read_scripts import read_dataset \n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import preprocessing\n",
    "\n",
    "task = \"remote_homology\"\n",
    "\n",
    "def fit_logistic(X, y):\n",
    "    start = timer()\n",
    "    clf = LogisticRegression(max_iter=5000)\n",
    "    clf.fit(X, y)\n",
    "    end = timer()\n",
    "    print(f\"fit time: \", timedelta(seconds=end-start))\n",
    "    \n",
    "    train_score = clf.score(X, y)\n",
    "    print(f\"model train score: \", train_score)\n",
    "    \n",
    "    # when run will play a ping sound!\n",
    "    os.system(\"printf '\\a'\")\n",
    "    os.system(\"printf '\\a'\")\n",
    "    os.system(\"printf '\\a'\")\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_2_arr(data_dict, labels, avgr=lambda x: np.mean(x, axis=0)):\n",
    "    \n",
    "    emb_shape = list(data_dict.values())[0].shape\n",
    "    number_of_embeddings = len(labels) \n",
    "\n",
    "    X = np.zeros((number_of_embeddings, emb_shape[-1]))\n",
    "    y = np.zeros(number_of_embeddings)\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    # iter over sorted keys in labels to ensure proteins\n",
    "    # from different models are indexed the same\n",
    "    keys = list(labels.keys())\n",
    "    keys.sort()\n",
    "    for key in keys :\n",
    "        if key == 'd1smyc_':\n",
    "            continue\n",
    "        X[i] = avgr(data_dict[key])\n",
    "        y[i] = labels[key]\n",
    "        i += 1\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_test_over_corpus_dict(corpus, model_name):\n",
    "\n",
    "    all_score = {}\n",
    "    labels = {}\n",
    "    \n",
    "    # read all labels\n",
    "    for split in ['train', 'valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']:\n",
    "        labels[split] = read_dataset(\"label\", \"remote_homology\", split)   \n",
    "\n",
    "    y_train_dict = labels['train']\n",
    "    X_train, y_train = dict_2_arr(corpus, labels['train'], avgr=lambda x: x)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "\n",
    "    # fit!\n",
    "    clf = fit_logistic(X_train, y_train)\n",
    "\n",
    "    # record train score\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    print(f\"{model_name} train score: \", train_score)\n",
    "\n",
    "    all_score[\"train\"]  = train_score\n",
    "\n",
    "    # get slices for remaing splits and score\n",
    "    remain_splits = ['valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']\n",
    "    for split in remain_splits:\n",
    "        \n",
    "        y_dict = labels[split]\n",
    "        X, y = dict_2_arr(corpus, labels[split], avgr=lambda x: x)\n",
    "        X = scaler.transform(X)\n",
    "        \n",
    "        print(split, X.shape, y.shape)\n",
    "        test_score = clf.score(X, y)\n",
    "\n",
    "        all_score[split]  = test_score\n",
    "\n",
    "        print(f\"{model_name} {split} score: \", test_score)\n",
    "\n",
    "    return all_score, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time:  0:28:09.303962\n",
      "model train score:  1.0\n",
      "Done!\n",
      "(12305, 3072) (12305,)\n",
      "CAEME_test train score:  1.0\n",
      "valid (734, 3072) (734,)\n",
      "CAEME_test valid score:  0.36239782016348776\n",
      "test_fold_holdout (718, 3072) (718,)\n",
      "CAEME_test test_fold_holdout score:  0.24651810584958217\n",
      "test_superfamily_holdout (1252, 3072) (1252,)\n",
      "CAEME_test test_superfamily_holdout score:  0.44329073482428116\n",
      "test_family_holdout (1272, 3072) (1272,)\n",
      "CAEME_test test_family_holdout score:  0.9316037735849056\n"
     ]
    }
   ],
   "source": [
    "scores, clf = train_test_over_corpus_dict(embeded_corpus, \"CAEME_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
