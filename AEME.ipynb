{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model of AEME\n",
    "# File: model.py\n",
    "# Author: Cong Bao\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as skpre\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "\n",
    "__author__ = 'Cong Bao'\n",
    "\n",
    "class AEME(object):\n",
    "    \"\"\" Autoencoded Meta-Embedding.\n",
    "        :param input_list: a list of source embedding path\n",
    "        :param output_path: a string path of output file\n",
    "        :param log_path: a string path of log file\n",
    "        :param ckpt_path: a string path of checkpoint file\n",
    "        :param model_type: the type of model, among DAEME, CAEME, AAEME\n",
    "        :param dims: a list of dimensionalities of each source embedding\n",
    "        :param learning_rate: a float number of the learning rate\n",
    "        :param batch_size: a int number of the batch size\n",
    "        :param epoch: a int number of the epoch\n",
    "        :param activ: a string name of activation function\n",
    "        :param factors: a list of coefficients of each loss part\n",
    "        :param noise: a float number between 0 and 1 of the masking noise rate\n",
    "        :param emb_dim: a int number of meta-embedding dimensionality, only used in AAEME model\n",
    "        :param oov: a boolean value whether to initialize inputs with oov or not\n",
    "        :param restore: a boolean value whether to restore checkpoint from local file or \n",
    "        :property logger: a logger to record log information\n",
    "        :property utils: a utility tool for I/O\n",
    "        :property sess: a tensorflow session\n",
    "        :property ckpt: a tensorflow saver \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model_type = kwargs['model']\n",
    "        self.dims = kwargs['dims']\n",
    "        self.learning_rate = kwargs['learning_rate']\n",
    "        self.batch_size = kwargs['batch']\n",
    "        self.epoch = kwargs['epoch']\n",
    "        self.activ = kwargs['activ']\n",
    "        self.factors = kwargs['factors']\n",
    "        self.noise = kwargs['noise']\n",
    "        self.emb_dim = kwargs['emb']\n",
    "        self.oov = kwargs['oov']\n",
    "        self.ckpt_path = kwargs['ckpt'] + 'model.ckpt'\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.ckpt = None\n",
    "\n",
    "    def load_data(self, src_dict_list, weights):\n",
    "        \"\"\" Load individual source embeddings and store intersection/union words and embeddings in separate lists.\n",
    "            If oov is True, word list will be the union of individual vocabularies.\n",
    "            If oov is False, word list will be the intersection of individual vocabularies.\n",
    "        \"\"\"\n",
    "\n",
    "        self.inter_words = list(set.intersection(*[set(src_dict.keys()) for src_dict in src_dict_list]))\n",
    "        print('Intersection Words: %s' % len(self.inter_words))\n",
    "        self.sources = np.asarray(list(zip(*[skpre.normalize([src_dict_list[i][word] for word in self.inter_words]) * weights[i] for i in range(len(src_dict_list)) ])))\n",
    "       \n",
    "        # delete the list of dicts to release memory\n",
    "        del src_dict_list\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\" Build the model of AEME.\n",
    "            The model to build will be one of DAEME, CAEME, or AAEME.\n",
    "        \"\"\"\n",
    "        # initialize sources and inputs\n",
    "        self.srcs = [tf.placeholder(tf.float32, (None, dim)) for dim in self.dims]\n",
    "        self.ipts = [tf.placeholder(tf.float32, (None, dim)) for dim in self.dims]\n",
    "        # select and build models\n",
    "        params = [self.dims, self.activ, self.factors]\n",
    "        if self.model_type == 'DAEME':\n",
    "            self.aeme = DAEME(*params)\n",
    "        elif self.model_type == 'CAEME':\n",
    "            self.aeme = CAEME(*params)\n",
    "        elif self.model_type == 'AAEME':\n",
    "            self.aeme = AAEME(*params, emb_dim=self.emb_dim)\n",
    "        self.aeme.build(self.srcs, self.ipts)\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\" Train the model.\n",
    "            Variables with least losses will be stored in checkpoint file.\n",
    "        \"\"\"\n",
    "        step = tf.Variable(0, trainable=False)\n",
    "        rate = tf.train.exponential_decay(self.learning_rate, step, 50, 0.99)\n",
    "        loss = self.aeme.loss()\n",
    "        opti = tf.train.AdamOptimizer(rate).minimize(loss, global_step=step)\n",
    "        self.ckpt = tf.train.Saver(tf.global_variables())\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        size = len(self.sources) // self.batch_size # the number of batches\n",
    "        best = float('inf')\n",
    "        # loop for N epoches\n",
    "        for itr in range(self.epoch):\n",
    "            indexes = np.random.permutation(len(self.sources)) # shuffle training inputs\n",
    "            train_loss = 0.\n",
    "            # train with mini-batches\n",
    "            for idx in range(size):\n",
    "                batch_idx = indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "                batches = list(zip(*self.sources[batch_idx]))\n",
    "                feed = {k:v for k, v in zip(self.srcs, batches)}\n",
    "                feed.update({k:self._corrupt(v) for k, v in zip(self.ipts, batches)})\n",
    "                _, batch_loss = self.sess.run([opti, loss], feed)\n",
    "                train_loss += batch_loss\n",
    "            epoch_loss = train_loss / size\n",
    "            # save the checkpoint with least loss\n",
    "            if epoch_loss <= best:\n",
    "                self.ckpt.save(self.sess, self.ckpt_path)\n",
    "                best = epoch_loss\n",
    "#             print('[Epoch{0}] loss: {1}'.format(itr, epoch_loss))\n",
    "\n",
    "    def generate_meta_embed(self):\n",
    "        \"\"\" Generate meta-embedding and save as local file.\n",
    "            Variables used to predict are these with least losses during training.\n",
    "        \"\"\"\n",
    "        embed= {}\n",
    "        print('Generating meta embeddings...')\n",
    "        self.ckpt.restore(self.sess, self.ckpt_path)\n",
    "        if self.oov:\n",
    "            vocabulary = self.union_words\n",
    "        else:\n",
    "            vocabulary = self.inter_words\n",
    "        for i, word in enumerate(vocabulary):\n",
    "            meta = self.sess.run(self.aeme.extract(), {k:[v] for k, v in zip(self.ipts, self.sources[i])})\n",
    "            embed[word] = np.reshape(meta, (np.shape(meta)[1],))\n",
    "        self.sess.close()\n",
    "        del self.sources\n",
    "        return embed\n",
    "\n",
    "    def _corrupt(self, batch):\n",
    "        \"\"\" Corrupt a batch using masking noises.\n",
    "            :param batch: the batch to be corrupted\n",
    "            :return: a new batch after corrupting\n",
    "        \"\"\"\n",
    "        noised = np.copy(batch)\n",
    "        batch_size, feature_size = np.shape(batch)\n",
    "        for i in range(batch_size):\n",
    "            mask = np.random.randint(0, feature_size, int(feature_size * self.noise))\n",
    "            for m in mask:\n",
    "                noised[i][m] = 0.\n",
    "        return noised\n",
    "\n",
    "class AbsModel(object):\n",
    "    \"\"\" Base class of all proposed methods.\n",
    "        :param dims: a list of dimensionalities of each input\n",
    "        :param activ: the string name of activation function\n",
    "        :param factors: a list of coefficients of each loss part\n",
    "    \"\"\"\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, dims, activ, factors):\n",
    "        self.dims = dims\n",
    "        self.factors = factors\n",
    "\n",
    "        if activ == 'lrelu':\n",
    "            self.activ = tf.keras.layers.LeakyReLU(0.2)\n",
    "        elif activ == 'prelu':\n",
    "            self.activ = tf.keras.layers.PReLU()\n",
    "        else:\n",
    "            self.activ = tf.keras.layers.Activation(activ)\n",
    "\n",
    "        self.meta = None\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(x, y, f):\n",
    "        \"\"\" Mean Squared Error with slicing.\n",
    "            This method will slice vector with higher dimension to the lower one,\n",
    "            if the two vector have different dimensions.\n",
    "            :param x: first vector\n",
    "            :param y: second vector\n",
    "            :param f: coefficient\n",
    "            :return: a tensor after calculating f * (1 / d) * ||x - y||^2\n",
    "        \"\"\"\n",
    "        x_d = x.get_shape().as_list()[1]\n",
    "        y_d = y.get_shape().as_list()[1]\n",
    "        if x_d != y_d:\n",
    "            smaller = min(x_d, y_d)\n",
    "            x = tf.slice(x, [0, 0], [tf.shape(x)[0], smaller])\n",
    "            y = tf.slice(y, [0, 0], [tf.shape(y)[0], smaller])\n",
    "        return tf.scalar_mul(f, tf.reduce_mean(tf.squared_difference(x, y)))\n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\" Extract the meta-embeddding model.\n",
    "            :return: the meta-embedding model\n",
    "        \"\"\"\n",
    "        return self.meta\n",
    "\n",
    "    @abstractmethod\n",
    "    def build(self, srcs, ipts):\n",
    "        \"\"\" Abstract method.\n",
    "            Build the model.\n",
    "            :param srcs: source embeddings\n",
    "            :param ipts: input embeddings\n",
    "        \"\"\"\n",
    "        self.srcs = srcs\n",
    "        self.ipts = ipts\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss(self):\n",
    "        \"\"\" Abstract method.\n",
    "            Obtain the loss function of model.\n",
    "            :return: a tensor calculating the loss function\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class DAEME(AbsModel):\n",
    "    \"\"\" Decoupled Autoencoded Meta-Embedding.\n",
    "        This method calculate meta-embedding as the concatenation of encoded source embeddings.\n",
    "        The loss function is defined as the sum of mse of each autoencoder and the mse between meta parts.\n",
    "    \"\"\"\n",
    "\n",
    "    def build(self, srcs, ipts):\n",
    "        AbsModel.build(self, srcs, ipts)\n",
    "        self.encoders = [tf.layers.dense(ipt, dim, self.activ) for ipt, dim in zip(self.ipts, self.dims)]\n",
    "        self.meta = tf.nn.l2_normalize(tf.concat(self.encoders, 1), 1)\n",
    "        self.outs = [tf.layers.dense(encoder, dim) for encoder, dim in zip(self.encoders, self.dims)]\n",
    "\n",
    "    def loss(self):\n",
    "        los = tf.add_n([self.mse(x, y, f) for x, y, f in zip(self.srcs, self.outs, self.factors[:-1])])\n",
    "        for x, y in combinations(self.encoders, 2):\n",
    "            los = tf.add(los, self.mse(x, y, self.factors[-1]))\n",
    "        return los\n",
    "\n",
    "class CAEME(AbsModel):\n",
    "    \"\"\" Concatenated Autoencoded Meta-Embedding.\n",
    "        This method calculate meta-embedding as the concatenation of encoded source embeddings.\n",
    "        The loss function is defined as the sum of mse of each autoencoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def build(self, srcs, ipts):\n",
    "        AbsModel.build(self, srcs, ipts)\n",
    "        self.encoders = [tf.layers.dense(ipt, dim, self.activ) for ipt, dim in zip(self.ipts, self.dims)]\n",
    "        self.meta = tf.nn.l2_normalize(tf.concat(self.encoders, 1), 1)\n",
    "        self.outs = [tf.layers.dense(self.meta, dim) for dim in self.dims]\n",
    "\n",
    "    def loss(self):\n",
    "        return tf.add_n([self.mse(x, y, f) for x, y, f in zip(self.srcs, self.outs, self.factors)])\n",
    "\n",
    "class AAEME(AbsModel):\n",
    "    \"\"\" Averaged Autoencoded Meta-Embedding.\n",
    "        This method calculate meta-embedding as the averaging of encoded source embeddings.\n",
    "        The loss function is defined as the sum of mse of each autoencoder.\n",
    "        :param emb_dim: the dimensionality of meta-embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        AbsModel.__init__(self, *args)\n",
    "        self.emb_dim = kwargs['emb_dim']\n",
    "\n",
    "    def build(self, srcs, ipts):\n",
    "        AbsModel.build(self, srcs, ipts)\n",
    "        self.encoders = [tf.layers.dense(ipt, self.emb_dim, self.activ) for ipt in self.ipts]\n",
    "        self.meta = tf.nn.l2_normalize(tf.add_n(self.encoders), 1)\n",
    "        self.outs = [tf.layers.dense(self.meta, dim) for dim in self.dims]\n",
    "\n",
    "    def loss(self):\n",
    "        return tf.add_n([self.mse(x, y, f) for x, y, f in zip(self.srcs, self.outs, self.factors)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_model_corpus_mean_dict(model):\n",
    "    \n",
    "    ds = []\n",
    "    # read all dicts\n",
    "    for split in ['train', 'valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']:\n",
    "\n",
    "        d = read_dataset(\"elmo\", \"remote_homology\", split)   \n",
    "        ds.append(d)\n",
    "\n",
    "    all_d = {}\n",
    "    \n",
    "    for d in ds:\n",
    "        for k in list(d.keys()):\n",
    "            seq = d[k]\n",
    "            all_d[k] = np.mean(seq, axis=0)\n",
    "\n",
    "    return all_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.pyplot import figure\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def spy_vis(arr, title_str=''):\n",
    "    fig = plt.figure(figsize=(5, 5), dpi=80)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.spy(arr)\n",
    "    plt.title(title_str)\n",
    "    plt.ylabel(\"Sample\")\n",
    "    plt.xlabel(\"Embedding Space\")\n",
    "    ax.set_aspect(aspect=0.2)\n",
    "    plt.show()\n",
    "\n",
    "def dict_to_arr_any(embeded_corpus):\n",
    "    emb_shape = list(embeded_corpus.values())[0].shape\n",
    "    number_of_embeddings = len(embeded_corpus) \n",
    "\n",
    "    embeded_corpus_arr = np.zeros((number_of_embeddings, emb_shape[-1]))\n",
    "\n",
    "    i = 0\n",
    "    keys = list(embeded_corpus.keys())\n",
    "    keys.sort()\n",
    "    for key in keys :\n",
    "        embeded_corpus_arr[i] = embeded_corpus[key]\n",
    "        i += 1\n",
    "    \n",
    "    return embeded_corpus_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_AEME(list_emb_dicts, weights, add_params):\n",
    "    dims = []\n",
    "    for d in list_emb_dicts:\n",
    "        dims.append(list(d.values())[0].shape[0])  \n",
    "\n",
    "    params = {}\n",
    "#     params['model'] = 'AAEME' # among DAEME, CAEME, AAEME\n",
    "    params['emb'] = 200 # a int number of meta-embedding dimensionality, only used in AAEME model\n",
    "    params['dims'] = dims # a list of encoded dimensionalities for each source embedding\n",
    "    params['learning_rate'] = 0.001\n",
    "    params['batch'] = 128\n",
    "    params['epoch'] = 200\n",
    "    params['activ'] = 'relu'\n",
    "    params['factors'] = [1.0, 1.0, 1.0]\n",
    "    params['noise'] = 0.00\n",
    "    params['oov'] = False\n",
    "    params['ckpt'] = './aeme'\n",
    "    \n",
    "    for key in list(add_params.keys()):\n",
    "        params[key] = add_params[key]\n",
    "    \n",
    "    aeme = AEME(**params)\n",
    "    print(f\"embedding for model: {params['model']} AAEME-emb: {params['emb']}\")\n",
    "    aeme.load_data(list_emb_dicts, weights)\n",
    "    aeme.build_model()\n",
    "    print(\"training...\")\n",
    "    aeme.train_model()\n",
    "    print(\"embedding...\")\n",
    "    embs = aeme.generate_meta_embed()\n",
    "    return embs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from read_scripts import dict_2_arr \n",
    "from read_scripts import read_dataset \n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import preprocessing\n",
    "\n",
    "task = \"remote_homology\"\n",
    "\n",
    "def fit_logistic(X, y):\n",
    "    start = timer()\n",
    "    clf = LogisticRegression(max_iter=5000)\n",
    "    clf.fit(X, y)\n",
    "    end = timer()\n",
    "    print(f\"fit time: \", timedelta(seconds=end-start))\n",
    "    \n",
    "    train_score = clf.score(X, y)\n",
    "    print(f\"model train score: \", train_score)\n",
    "    \n",
    "    # when run will play a ping sound!\n",
    "    os.system(\"printf '\\a'\")\n",
    "    os.system(\"printf '\\a'\")\n",
    "    os.system(\"printf '\\a'\")\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_2_arr(data_dict, labels, avgr=lambda x: np.mean(x, axis=0)):\n",
    "    \n",
    "    emb_shape = list(data_dict.values())[0].shape\n",
    "    number_of_embeddings = len(labels) \n",
    "\n",
    "    X = np.zeros((number_of_embeddings, emb_shape[-1]))\n",
    "    y = np.zeros(number_of_embeddings)\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    # iter over sorted keys in labels to ensure proteins\n",
    "    # from different models are indexed the same\n",
    "    keys = list(labels.keys())\n",
    "    keys.sort()\n",
    "    for key in keys :\n",
    "        if key == 'd1smyc_':\n",
    "            continue\n",
    "        X[i] = avgr(data_dict[key])\n",
    "        y[i] = labels[key]\n",
    "        i += 1\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "def normalize_dicts(dict_list):\n",
    "    res = []\n",
    "    for i in range(len(dict_list)):\n",
    "        cur_d = dict_list[i]\n",
    "        \n",
    "        for key in list(cur_d.keys()):\n",
    "            seq = cur_d[key]\n",
    "            seq = preprocessing.normalize(seq, norm='l2')\n",
    "            cur_d[key] = seq[0]\n",
    "            \n",
    "        res.append(cur_d)\n",
    "        \n",
    "    return res\n",
    "    \n",
    "def train_test_over_corpus_dict(corpus, model_name):\n",
    "\n",
    "    all_score = {}\n",
    "    labels = {}\n",
    "    \n",
    "    # read all labels\n",
    "    for split in ['train', 'valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']:\n",
    "        labels[split] = read_dataset(\"label\", \"remote_homology\", split)   \n",
    "\n",
    "    y_train_dict = labels['train']\n",
    "    X_train, y_train = dict_2_arr(corpus, labels['train'], avgr=lambda x: x)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "\n",
    "    # fit!\n",
    "    clf = fit_logistic(X_train, y_train)\n",
    "\n",
    "    # record train score\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    print(f\"{model_name} train score: \", train_score)\n",
    "\n",
    "    all_score[\"train\"]  = train_score\n",
    "\n",
    "    # get slices for remaing splits and score\n",
    "    remain_splits = ['valid', 'test_fold_holdout', 'test_superfamily_holdout', 'test_family_holdout']\n",
    "    for split in remain_splits:\n",
    "        \n",
    "        y_dict = labels[split]\n",
    "        X, y = dict_2_arr(corpus, labels[split], avgr=lambda x: x)\n",
    "        X = scaler.transform(X)\n",
    "        \n",
    "        print(split, X.shape, y.shape)\n",
    "        test_score = clf.score(X, y)\n",
    "\n",
    "        all_score[split]  = test_score\n",
    "\n",
    "        print(f\"{model_name} {split} score: \", test_score)\n",
    "\n",
    "    return all_score, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_scripts import read_dataset \n",
    "\n",
    "e_dict = get_single_model_corpus_mean_dict(\"elmo\")\n",
    "t_dict = get_single_model_corpus_mean_dict(\"transformer\")\n",
    "u_dict = get_single_model_corpus_mean_dict(\"unirep\")\n",
    "\n",
    "list_emb_dicts = [e_dict, t_dict, u_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_all_dim = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding for model: AAEME AAEME-emb: 10\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:03:54.927729\n",
      "model train score:  0.41015847216578627\n",
      "Done!\n",
      "(12305, 10) (12305,)\n",
      "AAEME dim:10 weights:[6, 3, 1] train score:  0.41015847216578627\n",
      "valid (734, 10) (734,)\n",
      "AAEME dim:10 weights:[6, 3, 1] valid score:  0.14713896457765668\n",
      "test_fold_holdout (718, 10) (718,)\n",
      "AAEME dim:10 weights:[6, 3, 1] test_fold_holdout score:  0.15459610027855153\n",
      "test_superfamily_holdout (1252, 10) (1252,)\n",
      "AAEME dim:10 weights:[6, 3, 1] test_superfamily_holdout score:  0.1757188498402556\n",
      "test_family_holdout (1272, 10) (1272,)\n",
      "AAEME dim:10 weights:[6, 3, 1] test_family_holdout score:  0.565251572327044\n",
      "embedding for model: AAEME AAEME-emb: 20\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:05:59.908320\n",
      "model train score:  0.6825680617635108\n",
      "Done!\n",
      "(12305, 20) (12305,)\n",
      "AAEME dim:20 weights:[6, 3, 1] train score:  0.6825680617635108\n",
      "valid (734, 20) (734,)\n",
      "AAEME dim:20 weights:[6, 3, 1] valid score:  0.23024523160762944\n",
      "test_fold_holdout (718, 20) (718,)\n",
      "AAEME dim:20 weights:[6, 3, 1] test_fold_holdout score:  0.17409470752089137\n",
      "test_superfamily_holdout (1252, 20) (1252,)\n",
      "AAEME dim:20 weights:[6, 3, 1] test_superfamily_holdout score:  0.2787539936102236\n",
      "test_family_holdout (1272, 20) (1272,)\n",
      "AAEME dim:20 weights:[6, 3, 1] test_family_holdout score:  0.7680817610062893\n",
      "embedding for model: AAEME AAEME-emb: 50\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:04:00.186118\n",
      "model train score:  0.9259650548557496\n",
      "Done!\n",
      "(12305, 50) (12305,)\n",
      "AAEME dim:50 weights:[6, 3, 1] train score:  0.9259650548557496\n",
      "valid (734, 50) (734,)\n",
      "AAEME dim:50 weights:[6, 3, 1] valid score:  0.3119891008174387\n",
      "test_fold_holdout (718, 50) (718,)\n",
      "AAEME dim:50 weights:[6, 3, 1] test_fold_holdout score:  0.21448467966573817\n",
      "test_superfamily_holdout (1252, 50) (1252,)\n",
      "AAEME dim:50 weights:[6, 3, 1] test_superfamily_holdout score:  0.36661341853035145\n",
      "test_family_holdout (1272, 50) (1272,)\n",
      "AAEME dim:50 weights:[6, 3, 1] test_family_holdout score:  0.8687106918238994\n",
      "embedding for model: AAEME AAEME-emb: 100\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:06:31.202504\n",
      "model train score:  0.9856156034132466\n",
      "Done!\n",
      "(12305, 100) (12305,)\n",
      "AAEME dim:100 weights:[6, 3, 1] train score:  0.9856156034132466\n",
      "valid (734, 100) (734,)\n",
      "AAEME dim:100 weights:[6, 3, 1] valid score:  0.3405994550408719\n",
      "test_fold_holdout (718, 100) (718,)\n",
      "AAEME dim:100 weights:[6, 3, 1] test_fold_holdout score:  0.233983286908078\n",
      "test_superfamily_holdout (1252, 100) (1252,)\n",
      "AAEME dim:100 weights:[6, 3, 1] test_superfamily_holdout score:  0.38977635782747605\n",
      "test_family_holdout (1272, 100) (1272,)\n",
      "AAEME dim:100 weights:[6, 3, 1] test_family_holdout score:  0.89937106918239\n",
      "embedding for model: AAEME AAEME-emb: 150\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:06:14.375097\n",
      "model train score:  0.9970743600162536\n",
      "Done!\n",
      "(12305, 150) (12305,)\n",
      "AAEME dim:150 weights:[6, 3, 1] train score:  0.9970743600162536\n",
      "valid (734, 150) (734,)\n",
      "AAEME dim:150 weights:[6, 3, 1] valid score:  0.3474114441416894\n",
      "test_fold_holdout (718, 150) (718,)\n",
      "AAEME dim:150 weights:[6, 3, 1] test_fold_holdout score:  0.25487465181058494\n",
      "test_superfamily_holdout (1252, 150) (1252,)\n",
      "AAEME dim:150 weights:[6, 3, 1] test_superfamily_holdout score:  0.4097444089456869\n",
      "test_family_holdout (1272, 150) (1272,)\n",
      "AAEME dim:150 weights:[6, 3, 1] test_family_holdout score:  0.9080188679245284\n",
      "embedding for model: AAEME AAEME-emb: 200\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:05:48.114279\n",
      "model train score:  0.9993498577813896\n",
      "Done!\n",
      "(12305, 200) (12305,)\n",
      "AAEME dim:200 weights:[6, 3, 1] train score:  0.9993498577813896\n",
      "valid (734, 200) (734,)\n",
      "AAEME dim:200 weights:[6, 3, 1] valid score:  0.35967302452316074\n",
      "test_fold_holdout (718, 200) (718,)\n",
      "AAEME dim:200 weights:[6, 3, 1] test_fold_holdout score:  0.23537604456824512\n",
      "test_superfamily_holdout (1252, 200) (1252,)\n",
      "AAEME dim:200 weights:[6, 3, 1] test_superfamily_holdout score:  0.41214057507987223\n",
      "test_family_holdout (1272, 200) (1272,)\n",
      "AAEME dim:200 weights:[6, 3, 1] test_family_holdout score:  0.9150943396226415\n",
      "embedding for model: AAEME AAEME-emb: 250\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:07:21.091870\n",
      "model train score:  0.9998374644453474\n",
      "Done!\n",
      "(12305, 250) (12305,)\n",
      "AAEME dim:250 weights:[6, 3, 1] train score:  0.9998374644453474\n",
      "valid (734, 250) (734,)\n",
      "AAEME dim:250 weights:[6, 3, 1] valid score:  0.37329700272479566\n",
      "test_fold_holdout (718, 250) (718,)\n",
      "AAEME dim:250 weights:[6, 3, 1] test_fold_holdout score:  0.22701949860724233\n",
      "test_superfamily_holdout (1252, 250) (1252,)\n",
      "AAEME dim:250 weights:[6, 3, 1] test_superfamily_holdout score:  0.4281150159744409\n",
      "test_family_holdout (1272, 250) (1272,)\n",
      "AAEME dim:250 weights:[6, 3, 1] test_family_holdout score:  0.9174528301886793\n",
      "embedding for model: AAEME AAEME-emb: 300\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:07:05.090853\n",
      "model train score:  1.0\n",
      "Done!\n",
      "(12305, 300) (12305,)\n",
      "AAEME dim:300 weights:[6, 3, 1] train score:  1.0\n",
      "valid (734, 300) (734,)\n",
      "AAEME dim:300 weights:[6, 3, 1] valid score:  0.3678474114441417\n",
      "test_fold_holdout (718, 300) (718,)\n",
      "AAEME dim:300 weights:[6, 3, 1] test_fold_holdout score:  0.233983286908078\n",
      "test_superfamily_holdout (1252, 300) (1252,)\n",
      "AAEME dim:300 weights:[6, 3, 1] test_superfamily_holdout score:  0.42012779552715657\n",
      "test_family_holdout (1272, 300) (1272,)\n",
      "AAEME dim:300 weights:[6, 3, 1] test_family_holdout score:  0.9127358490566038\n",
      "embedding for model: AAEME AAEME-emb: 350\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:07:23.240086\n",
      "model train score:  1.0\n",
      "Done!\n",
      "(12305, 350) (12305,)\n",
      "AAEME dim:350 weights:[6, 3, 1] train score:  1.0\n",
      "valid (734, 350) (734,)\n",
      "AAEME dim:350 weights:[6, 3, 1] valid score:  0.3569482288828338\n",
      "test_fold_holdout (718, 350) (718,)\n",
      "AAEME dim:350 weights:[6, 3, 1] test_fold_holdout score:  0.24233983286908078\n",
      "test_superfamily_holdout (1252, 350) (1252,)\n",
      "AAEME dim:350 weights:[6, 3, 1] test_superfamily_holdout score:  0.426517571884984\n",
      "test_family_holdout (1272, 350) (1272,)\n",
      "AAEME dim:350 weights:[6, 3, 1] test_family_holdout score:  0.9166666666666666\n",
      "embedding for model: AAEME AAEME-emb: 400\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:07:26.698569\n",
      "model train score:  1.0\n",
      "Done!\n",
      "(12305, 400) (12305,)\n",
      "AAEME dim:400 weights:[6, 3, 1] train score:  1.0\n",
      "valid (734, 400) (734,)\n",
      "AAEME dim:400 weights:[6, 3, 1] valid score:  0.36648501362397823\n",
      "test_fold_holdout (718, 400) (718,)\n",
      "AAEME dim:400 weights:[6, 3, 1] test_fold_holdout score:  0.23676880222841226\n",
      "test_superfamily_holdout (1252, 400) (1252,)\n",
      "AAEME dim:400 weights:[6, 3, 1] test_superfamily_holdout score:  0.43450479233226835\n",
      "test_family_holdout (1272, 400) (1272,)\n",
      "AAEME dim:400 weights:[6, 3, 1] test_family_holdout score:  0.9261006289308176\n",
      "embedding for model: AAEME AAEME-emb: 450\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:06:11.158017\n",
      "model train score:  1.0\n",
      "Done!\n",
      "(12305, 450) (12305,)\n",
      "AAEME dim:450 weights:[6, 3, 1] train score:  1.0\n",
      "valid (734, 450) (734,)\n",
      "AAEME dim:450 weights:[6, 3, 1] valid score:  0.35967302452316074\n",
      "test_fold_holdout (718, 450) (718,)\n",
      "AAEME dim:450 weights:[6, 3, 1] test_fold_holdout score:  0.24094707520891365\n",
      "test_superfamily_holdout (1252, 450) (1252,)\n",
      "AAEME dim:450 weights:[6, 3, 1] test_superfamily_holdout score:  0.43610223642172524\n",
      "test_family_holdout (1272, 450) (1272,)\n",
      "AAEME dim:450 weights:[6, 3, 1] test_family_holdout score:  0.9261006289308176\n",
      "embedding for model: AAEME AAEME-emb: 500\n",
      "Intersection Words: 16281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:06:12.546037\n",
      "model train score:  1.0\n",
      "Done!\n",
      "(12305, 500) (12305,)\n",
      "AAEME dim:500 weights:[6, 3, 1] train score:  1.0\n",
      "valid (734, 500) (734,)\n",
      "AAEME dim:500 weights:[6, 3, 1] valid score:  0.36648501362397823\n",
      "test_fold_holdout (718, 500) (718,)\n",
      "AAEME dim:500 weights:[6, 3, 1] test_fold_holdout score:  0.22701949860724233\n",
      "test_superfamily_holdout (1252, 500) (1252,)\n",
      "AAEME dim:500 weights:[6, 3, 1] test_superfamily_holdout score:  0.43690095846645366\n",
      "test_family_holdout (1272, 500) (1272,)\n",
      "AAEME dim:500 weights:[6, 3, 1] test_family_holdout score:  0.9323899371069182\n",
      "embedding for model: AAEME AAEME-emb: 550\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:05:16.158854\n",
      "model train score:  1.0\n",
      "Done!\n",
      "(12305, 550) (12305,)\n",
      "AAEME dim:550 weights:[6, 3, 1] train score:  1.0\n",
      "valid (734, 550) (734,)\n",
      "AAEME dim:550 weights:[6, 3, 1] valid score:  0.36239782016348776\n",
      "test_fold_holdout (718, 550) (718,)\n",
      "AAEME dim:550 weights:[6, 3, 1] test_fold_holdout score:  0.23537604456824512\n",
      "test_superfamily_holdout (1252, 550) (1252,)\n",
      "AAEME dim:550 weights:[6, 3, 1] test_superfamily_holdout score:  0.43849840255591055\n",
      "test_family_holdout (1272, 550) (1272,)\n",
      "AAEME dim:550 weights:[6, 3, 1] test_family_holdout score:  0.9323899371069182\n",
      "embedding for model: AAEME AAEME-emb: 600\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:04:57.934840\n",
      "model train score:  1.0\n",
      "Done!\n",
      "(12305, 600) (12305,)\n",
      "AAEME dim:600 weights:[6, 3, 1] train score:  1.0\n",
      "valid (734, 600) (734,)\n",
      "AAEME dim:600 weights:[6, 3, 1] valid score:  0.3692098092643052\n",
      "test_fold_holdout (718, 600) (718,)\n",
      "AAEME dim:600 weights:[6, 3, 1] test_fold_holdout score:  0.24233983286908078\n",
      "test_superfamily_holdout (1252, 600) (1252,)\n",
      "AAEME dim:600 weights:[6, 3, 1] test_superfamily_holdout score:  0.4416932907348243\n",
      "test_family_holdout (1272, 600) (1272,)\n",
      "AAEME dim:600 weights:[6, 3, 1] test_family_holdout score:  0.9323899371069182\n",
      "embedding for model: AAEME AAEME-emb: 650\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:04:57.477408\n",
      "model train score:  1.0\n",
      "Done!\n",
      "(12305, 650) (12305,)\n",
      "AAEME dim:650 weights:[6, 3, 1] train score:  1.0\n",
      "valid (734, 650) (734,)\n",
      "AAEME dim:650 weights:[6, 3, 1] valid score:  0.3583106267029973\n",
      "test_fold_holdout (718, 650) (718,)\n",
      "AAEME dim:650 weights:[6, 3, 1] test_fold_holdout score:  0.24233983286908078\n",
      "test_superfamily_holdout (1252, 650) (1252,)\n",
      "AAEME dim:650 weights:[6, 3, 1] test_superfamily_holdout score:  0.43769968051118213\n",
      "test_family_holdout (1272, 650) (1272,)\n",
      "AAEME dim:650 weights:[6, 3, 1] test_family_holdout score:  0.934748427672956\n",
      "embedding for model: AAEME AAEME-emb: 700\n",
      "Intersection Words: 16281\n",
      "training...\n",
      "embedding...\n",
      "Generating meta embeddings...\n",
      "INFO:tensorflow:Restoring parameters from ./aememodel.ckpt\n",
      "fit time:  0:05:07.622484\n",
      "model train score:  1.0\n",
      "Done!\n",
      "(12305, 700) (12305,)\n",
      "AAEME dim:700 weights:[6, 3, 1] train score:  1.0\n",
      "valid (734, 700) (734,)\n",
      "AAEME dim:700 weights:[6, 3, 1] valid score:  0.36648501362397823\n",
      "test_fold_holdout (718, 700) (718,)\n",
      "AAEME dim:700 weights:[6, 3, 1] test_fold_holdout score:  0.23676880222841226\n",
      "test_superfamily_holdout (1252, 700) (1252,)\n",
      "AAEME dim:700 weights:[6, 3, 1] test_superfamily_holdout score:  0.4329073482428115\n",
      "test_family_holdout (1272, 700) (1272,)\n",
      "AAEME dim:700 weights:[6, 3, 1] test_family_holdout score:  0.9292452830188679\n"
     ]
    }
   ],
   "source": [
    "weights = [6,3,1]\n",
    "params = {}\n",
    "params['model'] = 'AAEME'\n",
    "dims = [10, 20, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700]\n",
    "for dim in dims:\n",
    "    params['emb'] = dim\n",
    "    embeded_corpus = embed_AEME(list_emb_dicts_wn, weights, params)\n",
    "    scores, clf = train_test_over_corpus_dict(embeded_corpus, f\"AAEME dim:{dim} weights:{weights}\")\n",
    "    all_scores_all_dim[str(dim)] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"AAEME_scores_across_dims_weighted_{weights}.p\", \"wb\") as f:\n",
    "    pickle.dump(all_scores_all_dim, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_scripts import read_dataset \n",
    "\n",
    "e_dict = get_single_model_corpus_mean_dict(\"elmo\")\n",
    "t_dict = get_single_model_corpus_mean_dict(\"transformer\")\n",
    "u_dict = get_single_model_corpus_mean_dict(\"unirep\")\n",
    "\n",
    "list_emb_dicts = [e_dict, t_dict, u_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding for model: DAEME AAEME-emb: 200\n",
      "Intersection Words: 16281\n",
      "training...\n"
     ]
    }
   ],
   "source": [
    "weights = [6,3,1]\n",
    "params = {}\n",
    "params['model'] = 'DAEME'\n",
    "\n",
    "embeded_corpus = embed_AEME(list_emb_dicts_wn, weights, params)\n",
    "scores, clf = train_test_over_corpus_dict(embeded_corpus, f\"DAEME dim:{dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
